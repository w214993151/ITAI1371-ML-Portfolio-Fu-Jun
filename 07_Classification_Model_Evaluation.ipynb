{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Model Evaluation\n",
    "## Module 6-7, Lab 4: Building and Evaluating Classification Models\n",
    "\n",
    "Classification is one of the most common machine learning tasks. In this lab, you'll learn how to build classification models and evaluate their performance using various metrics and techniques.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Build logistic regression and decision tree classifiers\n",
    "- Understand the difference between regression and classification\n",
    "- Evaluate classification models using multiple metrics\n",
    "- Interpret confusion matrices and ROC curves\n",
    "- Use cross-validation for robust model assessment\n",
    "- Handle class imbalance in datasets\n",
    "\n",
    "### Business Problem\n",
    "We'll predict employee attrition (whether an employee will leave the company) based on various factors. This is crucial for HR departments to identify at-risk employees and take preventive measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Employee Attrition Dataset\n",
    "We'll create a realistic dataset for predicting employee attrition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a realistic employee attrition dataset\n",
    "np.random.seed(42)\n",
    "n_employees = 1500\n",
    "\n",
    "# Generate employee features\n",
    "employee_data = {\n",
    "    'age': np.random.normal(35, 10, n_employees),\n",
    "    'years_at_company': np.random.exponential(4, n_employees),\n",
    "    'salary': np.random.normal(75000, 20000, n_employees),\n",
    "    'satisfaction_score': np.random.normal(7, 2, n_employees),\n",
    "    'performance_rating': np.random.normal(3.5, 0.8, n_employees),\n",
    "    'work_life_balance': np.random.normal(3, 1, n_employees),\n",
    "    'commute_distance': np.random.exponential(15, n_employees),\n",
    "    'overtime_hours': np.random.exponential(5, n_employees),\n",
    "    'department': np.random.choice(['Engineering', 'Sales', 'Marketing', 'HR', 'Finance'], \n",
    "                                  n_employees, p=[0.35, 0.25, 0.15, 0.15, 0.1]),\n",
    "    'job_level': np.random.choice(['Junior', 'Mid', 'Senior', 'Manager'], \n",
    "                                 n_employees, p=[0.3, 0.4, 0.25, 0.05]),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "                                 n_employees, p=[0.15, 0.5, 0.3, 0.05]),\n",
    "    'remote_work': np.random.choice([0, 1], n_employees, p=[0.7, 0.3]),\n",
    "    'has_promotion_last_year': np.random.choice([0, 1], n_employees, p=[0.85, 0.15])\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(employee_data)\n",
    "\n",
    "# Apply realistic constraints\n",
    "df['age'] = np.clip(df['age'], 22, 65)\n",
    "df['years_at_company'] = np.clip(df['years_at_company'], 0, df['age'] - 22)\n",
    "df['salary'] = np.clip(df['salary'], 35000, 150000)\n",
    "df['satisfaction_score'] = np.clip(df['satisfaction_score'], 1, 10)\n",
    "df['performance_rating'] = np.clip(df['performance_rating'], 1, 5)\n",
    "df['work_life_balance'] = np.clip(df['work_life_balance'], 1, 5)\n",
    "df['commute_distance'] = np.clip(df['commute_distance'], 1, 50)\n",
    "df['overtime_hours'] = np.clip(df['overtime_hours'], 0, 20)\n",
    "\n",
    "print(f\"Dataset created with {len(df)} employees\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Target Variable (Attrition)\n",
    "We'll create a realistic attrition target based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attrition probability based on realistic factors\n",
    "# Lower satisfaction, poor work-life balance, long commute, excessive overtime increase attrition\n",
    "attrition_probability = (\n",
    "    0.1 +  # Base probability\n",
    "    (10 - df['satisfaction_score']) * 0.05 +  # Lower satisfaction increases risk\n",
    "    (5 - df['work_life_balance']) * 0.03 +    # Poor work-life balance increases risk\n",
    "    (df['commute_distance'] / 50) * 0.15 +    # Longer commute increases risk\n",
    "    (df['overtime_hours'] / 20) * 0.1 +       # More overtime increases risk\n",
    "    (df['years_at_company'] < 1) * 0.2 +      # New employees more likely to leave\n",
    "    (df['years_at_company'] > 10) * 0.05 +    # Very tenured employees slightly more likely to leave\n",
    "    (df['performance_rating'] < 2.5) * 0.15 + # Poor performers more likely to leave\n",
    "    (df['has_promotion_last_year'] == 0) * 0.05  # No recent promotion increases risk\n",
    ")\n",
    "\n",
    "# Ensure probabilities are between 0 and 1\n",
    "attrition_probability = np.clip(attrition_probability, 0, 0.8)\n",
    "\n",
    "# Generate binary attrition outcome\n",
    "df['attrition'] = np.random.binomial(1, attrition_probability)\n",
    "\n",
    "print(f\"Attrition rate: {df['attrition'].mean():.2%}\")\n",
    "print(f\"Employees who left: {df['attrition'].sum()}\")\n",
    "print(f\"Employees who stayed: {len(df) - df['attrition'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Exploratory Data Analysis for Classification\n",
    "Let's understand the relationship between features and attrition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics by attrition status\n",
    "print(\"=== ATTRITION ANALYSIS ===\")\n",
    "print(f\"\\nðŸ“Š Overall Statistics:\")\n",
    "print(f\"   â€¢ Total employees: {len(df):,}\")\n",
    "print(f\"   â€¢ Attrition rate: {df['attrition'].mean():.1%}\")\n",
    "print(f\"   â€¢ Employees who left: {df['attrition'].sum():,}\")\n",
    "print(f\"   â€¢ Employees who stayed: {(df['attrition'] == 0).sum():,}\")\n",
    "\n",
    "# Compare numerical features by attrition status\n",
    "numerical_cols = ['age', 'years_at_company', 'salary', 'satisfaction_score', \n",
    "                 'performance_rating', 'work_life_balance', 'commute_distance', 'overtime_hours']\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Average Values by Attrition Status:\")\n",
    "comparison = df.groupby('attrition')[numerical_cols].mean()\n",
    "print(comparison.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the differences between groups\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    # Box plot comparing attrition groups\n",
    "    df.boxplot(column=col, by='attrition', ax=axes[i])\n",
    "    axes[i].set_title(f'{col.replace(\"_\", \" \").title()} by Attrition')\n",
    "    axes[i].set_xlabel('Attrition (0=Stayed, 1=Left)')\n",
    "    axes[i].set_ylabel(col.replace(\"_\", \" \").title())\n",
    "\n",
    "plt.suptitle('Feature Distributions by Attrition Status', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical variables\n",
    "categorical_cols = ['department', 'job_level', 'education', 'remote_work', 'has_promotion_last_year']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    # Calculate attrition rate by category\n",
    "    attrition_by_cat = df.groupby(col)['attrition'].agg(['count', 'sum', 'mean']).round(3)\n",
    "    attrition_by_cat.columns = ['Total', 'Left', 'Attrition_Rate']\n",
    "    \n",
    "    # Plot attrition rate\n",
    "    axes[i].bar(attrition_by_cat.index.astype(str), attrition_by_cat['Attrition_Rate'])\n",
    "    axes[i].set_title(f'Attrition Rate by {col.replace(\"_\", \" \").title()}')\n",
    "    axes[i].set_ylabel('Attrition Rate')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, v in enumerate(attrition_by_cat['Attrition_Rate']):\n",
    "        axes[i].text(j, v + 0.01, f'{v:.2%}', ha='center', va='bottom')\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed breakdown\n",
    "print(\"\\nDetailed Attrition Analysis by Category:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col.replace('_', ' ').title()}:\")\n",
    "    attrition_by_cat = df.groupby(col)['attrition'].agg(['count', 'sum', 'mean'])\n",
    "    attrition_by_cat.columns = ['Total', 'Left', 'Attrition_Rate']\n",
    "    attrition_by_cat['Attrition_Rate'] = attrition_by_cat['Attrition_Rate'].apply(lambda x: f\"{x:.1%}\")\n",
    "    print(attrition_by_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preparation for Classification\n",
    "Let's prepare our data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "# Select features for modeling\n",
    "feature_columns = numerical_cols + categorical_cols\n",
    "X = df[feature_columns]\n",
    "y = df['attrition']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nClass balance: {y.value_counts(normalize=True).round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical_features = numerical_cols\n",
    "categorical_features = categorical_cols\n",
    "\n",
    "# Create preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed training set shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed test set shape: {X_test_processed.shape}\")\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "num_feature_names = numerical_features\n",
    "cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "all_feature_names = num_feature_names + list(cat_feature_names)\n",
    "print(f\"\\nTotal features after preprocessing: {len(all_feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Building Classification Models\n",
    "Let's build and compare different classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train_processed)\n",
    "y_test_pred_lr = lr_model.predict(X_test_processed)\n",
    "y_test_pred_proba_lr = lr_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "print(\"Logistic Regression Model Trained!\")\n",
    "print(f\"Training accuracy: {accuracy_score(y_train, y_train_pred_lr):.3f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred_lr):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance in logistic regression\n",
    "feature_importance_lr = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'coefficient': lr_model.coef_[0],\n",
    "    'abs_coefficient': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (Logistic Regression):\")\n",
    "print(feature_importance_lr.head(10).round(4))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance_lr.head(10)\n",
    "colors = ['red' if x < 0 else 'green' for x in top_features['coefficient']]\n",
    "plt.barh(range(len(top_features)), top_features['coefficient'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Top 10 Feature Coefficients in Logistic Regression')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=6, min_samples_split=50, min_samples_leaf=20)\n",
    "dt_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_dt = dt_model.predict(X_train_processed)\n",
    "y_test_pred_dt = dt_model.predict(X_test_processed)\n",
    "y_test_pred_proba_dt = dt_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "print(\"Decision Tree Model Trained!\")\n",
    "print(f\"Training accuracy: {accuracy_score(y_train, y_train_pred_dt):.3f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred_dt):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance in decision tree\n",
    "feature_importance_dt = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'importance': dt_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (Decision Tree):\")\n",
    "print(feature_importance_dt.head(10).round(4))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features_dt = feature_importance_dt.head(10)\n",
    "plt.barh(range(len(top_features_dt)), top_features_dt['importance'], alpha=0.7)\n",
    "plt.yticks(range(len(top_features_dt)), top_features_dt['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 Feature Importances in Decision Tree')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a simplified decision tree\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(dt_model, \n",
    "          feature_names=all_feature_names,\n",
    "          class_names=['Stayed', 'Left'],\n",
    "          filled=True,\n",
    "          max_depth=3,  # Show only top 3 levels for readability\n",
    "          fontsize=10)\n",
    "plt.title('Decision Tree Visualization (Top 3 Levels)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Evaluation\n",
    "Let's comprehensively evaluate our models using various metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for both models\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1-Score': f1_score(y_true, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_true, y_pred_proba)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Evaluate both models\n",
    "lr_metrics = evaluate_model(y_test, y_test_pred_lr, y_test_pred_proba_lr, 'Logistic Regression')\n",
    "dt_metrics = evaluate_model(y_test, y_test_pred_dt, y_test_pred_proba_dt, 'Decision Tree')\n",
    "\n",
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame([lr_metrics, dt_metrics])\n",
    "results_df = results_df.set_index('Model')\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Visualize the comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "results_df.plot(kind='bar', ax=ax, alpha=0.8)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Logistic Regression Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_test_pred_lr)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Stayed', 'Left'], yticklabels=['Stayed', 'Left'])\n",
    "axes[0].set_title('Logistic Regression\\nConfusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Decision Tree Confusion Matrix\n",
    "cm_dt = confusion_matrix(y_test, y_test_pred_dt)\n",
    "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Stayed', 'Left'], yticklabels=['Stayed', 'Left'])\n",
    "axes[1].set_title('Decision Tree\\nConfusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpret confusion matrices\n",
    "print(\"Confusion Matrix Interpretation:\")\n",
    "print(\"\\nLogistic Regression:\")\n",
    "tn_lr, fp_lr, fn_lr, tp_lr = cm_lr.ravel()\n",
    "print(f\"  True Negatives (Correctly predicted stayed): {tn_lr}\")\n",
    "print(f\"  False Positives (Incorrectly predicted left): {fp_lr}\")\n",
    "print(f\"  False Negatives (Incorrectly predicted stayed): {fn_lr}\")\n",
    "print(f\"  True Positives (Correctly predicted left): {tp_lr}\")\n",
    "\n",
    "print(\"\\nDecision Tree:\")\n",
    "tn_dt, fp_dt, fn_dt, tp_dt = cm_dt.ravel()\n",
    "print(f\"  True Negatives (Correctly predicted stayed): {tn_dt}\")\n",
    "print(f\"  False Positives (Incorrectly predicted left): {fp_dt}\")\n",
    "print(f\"  False Negatives (Incorrectly predicted stayed): {fn_dt}\")\n",
    "print(f\"  True Positives (Correctly predicted left): {tp_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Logistic Regression ROC\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_test_pred_proba_lr)\n",
    "auc_lr = roc_auc_score(y_test, y_test_pred_proba_lr)\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.3f})', linewidth=2)\n",
    "\n",
    "# Decision Tree ROC\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_test_pred_proba_dt)\n",
    "auc_dt = roc_auc_score(y_test, y_test_pred_proba_dt)\n",
    "plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {auc_dt:.3f})', linewidth=2)\n",
    "\n",
    "# Random classifier line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.500)', alpha=0.5)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"ROC Curve Interpretation:\")\n",
    "print(\"â€¢ AUC = 0.5: Random classifier (no predictive power)\")\n",
    "print(\"â€¢ AUC = 1.0: Perfect classifier\")\n",
    "print(\"â€¢ Higher AUC = Better model performance\")\n",
    "print(\"â€¢ The curve closer to the top-left corner is better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Detailed Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification reports\n",
    "print(\"=== DETAILED CLASSIFICATION REPORTS ===\")\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print(classification_report(y_test, y_test_pred_lr, target_names=['Stayed', 'Left']))\n",
    "\n",
    "print(\"\\nDecision Tree:\")\n",
    "print(classification_report(y_test, y_test_pred_dt, target_names=['Stayed', 'Left']))\n",
    "\n",
    "print(\"\\n=== METRIC EXPLANATIONS ===\")\n",
    "print(\"â€¢ Precision: Of all predicted positives, how many were actually positive?\")\n",
    "print(\"â€¢ Recall: Of all actual positives, how many did we correctly identify?\")\n",
    "print(\"â€¢ F1-Score: Harmonic mean of precision and recall\")\n",
    "print(\"â€¢ Support: Number of actual occurrences of each class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Cross-Validation\n",
    "Let's use cross-validation for more robust model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation for Logistic Regression\n",
    "cv_scores_lr = cross_val_score(lr_model, X_train_processed, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "# Cross-validation for Decision Tree\n",
    "cv_scores_dt = cross_val_score(dt_model, X_train_processed, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(\"Cross-Validation Results (ROC-AUC):\")\n",
    "print(f\"\\nLogistic Regression:\")\n",
    "print(f\"  Individual fold scores: {cv_scores_lr.round(4)}\")\n",
    "print(f\"  Mean CV score: {cv_scores_lr.mean():.4f} (+/- {cv_scores_lr.std() * 2:.4f})\")\n",
    "\n",
    "print(f\"\\nDecision Tree:\")\n",
    "print(f\"  Individual fold scores: {cv_scores_dt.round(4)}\")\n",
    "print(f\"  Mean CV score: {cv_scores_dt.mean():.4f} (+/- {cv_scores_dt.std() * 2:.4f})\")\n",
    "\n",
    "# Visualize cross-validation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([cv_scores_lr, cv_scores_dt], labels=['Logistic Regression', 'Decision Tree'])\n",
    "plt.ylabel('ROC-AUC Score')\n",
    "plt.title('Cross-Validation Performance Comparison')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Interpretation and Business Insights\n",
    "Let's extract actionable insights from our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction probabilities\n",
    "# Create risk categories based on predicted probabilities\n",
    "def categorize_risk(prob):\n",
    "    if prob < 0.3:\n",
    "        return 'Low Risk'\n",
    "    elif prob < 0.6:\n",
    "        return 'Medium Risk'\n",
    "    else:\n",
    "        return 'High Risk'\n",
    "\n",
    "# Apply to logistic regression predictions (generally more calibrated)\n",
    "risk_categories = [categorize_risk(p) for p in y_test_pred_proba_lr]\n",
    "risk_df = pd.DataFrame({\n",
    "    'actual_attrition': y_test,\n",
    "    'predicted_probability': y_test_pred_proba_lr,\n",
    "    'risk_category': risk_categories\n",
    "})\n",
    "\n",
    "# Analyze risk categories\n",
    "risk_analysis = risk_df.groupby('risk_category').agg({\n",
    "    'actual_attrition': ['count', 'sum', 'mean'],\n",
    "    'predicted_probability': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "risk_analysis.columns = ['Total_Employees', 'Actual_Attrition', 'Attrition_Rate', 'Avg_Predicted_Prob']\n",
    "\n",
    "print(\"Risk Category Analysis:\")\n",
    "print(risk_analysis)\n",
    "\n",
    "# Visualize risk categories\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Risk category distribution\n",
    "risk_counts = risk_df['risk_category'].value_counts()\n",
    "axes[0].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Distribution of Risk Categories')\n",
    "\n",
    "# Attrition rate by risk category\n",
    "axes[1].bar(risk_analysis.index, risk_analysis['Attrition_Rate'])\n",
    "axes[1].set_title('Actual Attrition Rate by Risk Category')\n",
    "axes[1].set_ylabel('Attrition Rate')\n",
    "axes[1].set_xlabel('Risk Category')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(risk_analysis['Attrition_Rate']):\n",
    "    axes[1].text(i, v + 0.01, f'{v:.1%}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business insights and recommendations\n",
    "print(\"=== BUSINESS INSIGHTS AND RECOMMENDATIONS ===\")\n",
    "\n",
    "# Top risk factors from logistic regression\n",
    "top_risk_factors = feature_importance_lr.head(5)\n",
    "print(\"\\nðŸŽ¯ Top 5 Attrition Risk Factors:\")\n",
    "for idx, row in top_risk_factors.iterrows():\n",
    "    direction = \"increases\" if row['coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"   â€¢ {row['feature']}: {direction} attrition risk (coef: {row['coefficient']:.3f})\")\n",
    "\n",
    "# Model performance summary\n",
    "print(f\"\\nðŸ“Š Model Performance Summary:\")\n",
    "best_model = \"Logistic Regression\" if auc_lr > auc_dt else \"Decision Tree\"\n",
    "best_auc = max(auc_lr, auc_dt)\n",
    "print(f\"   â€¢ Best performing model: {best_model} (AUC: {best_auc:.3f})\")\n",
    "print(f\"   â€¢ Model can identify {risk_analysis.loc['High Risk', 'Attrition_Rate']:.1%} of high-risk employees\")\n",
    "print(f\"   â€¢ {risk_analysis.loc['High Risk', 'Total_Employees']} employees classified as high-risk\")\n",
    "\n",
    "# Actionable recommendations\n",
    "print(f\"\\nðŸ’¡ Actionable Recommendations:\")\n",
    "print(f\"   1. Focus retention efforts on {risk_analysis.loc['High Risk', 'Total_Employees']} high-risk employees\")\n",
    "print(f\"   2. Address low satisfaction scores - strongest predictor of attrition\")\n",
    "print(f\"   3. Improve work-life balance programs\")\n",
    "print(f\"   4. Consider remote work options to reduce commute impact\")\n",
    "print(f\"   5. Monitor new employees closely (< 1 year tenure)\")\n",
    "print(f\"   6. Implement regular performance feedback and development programs\")\n",
    "\n",
    "# Cost-benefit analysis\n",
    "avg_replacement_cost = 50000  # Typical cost to replace an employee\n",
    "high_risk_employees = risk_analysis.loc['High Risk', 'Total_Employees']\n",
    "expected_attrition = high_risk_employees * risk_analysis.loc['High Risk', 'Attrition_Rate']\n",
    "potential_savings = expected_attrition * avg_replacement_cost\n",
    "\n",
    "print(f\"\\nðŸ’° Potential Cost Savings:\")\n",
    "print(f\"   â€¢ Expected attrition from high-risk group: {expected_attrition:.0f} employees\")\n",
    "print(f\"   â€¢ Potential cost savings if 50% retention improvement: ${potential_savings * 0.5:,.0f}\")\n",
    "print(f\"   â€¢ ROI of targeted retention programs could be substantial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Your Turn to Practice!\n",
    "Now it's your turn to apply classification techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Feature Engineering\n",
    "Create a new feature called 'tenure_satisfaction_ratio' (years_at_company / satisfaction_score) and retrain the logistic regression model. Does this improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 1\n",
    "# Hint: Create the new feature, add it to your feature set, and retrain the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Threshold Optimization\n",
    "The default classification threshold is 0.5. Find the optimal threshold that maximizes the F1-score for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 2\n",
    "# Hint: Try different thresholds from 0.1 to 0.9 and calculate F1-score for each\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Class Imbalance\n",
    "Our dataset has class imbalance. Try using class weights in logistic regression to handle this. Compare the results with the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 3\n",
    "# Hint: Use class_weight='balanced' parameter in LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've mastered the fundamentals of classification and model evaluation. Here's what you've learned:\n",
    "\n",
    "### âœ… Key Skills Mastered:\n",
    "1. **Classification Algorithms**: Logistic regression and decision trees\n",
    "2. **Model Evaluation**: Accuracy, precision, recall, F1-score, ROC-AUC\n",
    "3. **Confusion Matrices**: Understanding true/false positives and negatives\n",
    "4. **ROC Curves**: Visualizing model performance across thresholds\n",
    "5. **Cross-Validation**: Robust model assessment using multiple folds\n",
    "6. **Feature Importance**: Understanding which features drive predictions\n",
    "7. **Business Application**: Translating model results into actionable insights\n",
    "\n",
    "### ðŸ” Key Concepts Learned:\n",
    "- **Precision vs Recall Trade-off**: High precision = fewer false alarms, high recall = catch more positives\n",
    "- **ROC-AUC**: Measures model's ability to distinguish between classes\n",
    "- **Cross-Validation**: Provides more reliable performance estimates\n",
    "- **Feature Interpretation**: Logistic regression coefficients show feature impact direction\n",
    "- **Business Value**: Models must translate to actionable business decisions\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "In the next lab, we'll explore ensemble methods that combine multiple models to achieve better performance:\n",
    "- Random Forests (bagging)\n",
    "- Gradient Boosting (boosting)\n",
    "- Model stacking and voting\n",
    "- Hyperparameter tuning\n",
    "\n",
    "### ðŸ“š Additional Resources:\n",
    "- [Scikit-learn Classification Guide](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n",
    "- [Understanding ROC Curves](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)\n",
    "- [Precision vs Recall](https://towardsdatascience.com/precision-vs-recall-386cf9f89488)\n",
    "- [Cross-Validation Explained](https://towardsdatascience.com/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

