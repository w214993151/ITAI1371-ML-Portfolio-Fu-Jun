{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "## Module 5, Lab 3: Preparing Data for Machine Learning\n",
    "\n",
    "Raw data is rarely ready for machine learning algorithms. This lab teaches you how to clean, transform, and engineer features to create high-quality datasets that lead to better model performance.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Handle missing values using various strategies\n",
    "- Encode categorical variables for machine learning\n",
    "- Scale and normalize numerical features\n",
    "- Detect and handle outliers appropriately\n",
    "- Create new features through feature engineering\n",
    "- Build preprocessing pipelines for reproducibility\n",
    "\n",
    "### Why This Matters\n",
    "Data preprocessing often takes 80% of a data scientist's time, but it's crucial for model success. Poor data preparation leads to poor models, regardless of the algorithm used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Realistic Dataset with Data Quality Issues\n",
    "We'll create a dataset that mimics real-world data problems you'll encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a realistic employee dataset with various data quality issues\n",
    "np.random.seed(42)\n",
    "n_employees = 1000\n",
    "\n",
    "# Generate base employee data\n",
    "employee_data = {\n",
    "    'employee_id': range(1, n_employees + 1),\n",
    "    'age': np.random.normal(35, 10, n_employees),\n",
    "    'years_experience': np.random.exponential(5, n_employees),\n",
    "    'education_level': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "                                       n_employees, p=[0.2, 0.5, 0.25, 0.05]),\n",
    "    'department': np.random.choice(['Engineering', 'Sales', 'Marketing', 'HR', 'Finance'], \n",
    "                                  n_employees, p=[0.3, 0.25, 0.2, 0.15, 0.1]),\n",
    "    'job_level': np.random.choice(['Junior', 'Mid', 'Senior', 'Lead'], \n",
    "                                 n_employees, p=[0.3, 0.4, 0.25, 0.05]),\n",
    "    'location': np.random.choice(['New York', 'San Francisco', 'Chicago', 'Austin', 'Remote'], \n",
    "                                n_employees, p=[0.25, 0.2, 0.15, 0.15, 0.25]),\n",
    "    'performance_score': np.random.normal(7.5, 1.5, n_employees),\n",
    "    'hours_per_week': np.random.normal(42, 8, n_employees),\n",
    "    'projects_completed': np.random.poisson(8, n_employees),\n",
    "    'training_hours': np.random.gamma(2, 10, n_employees)\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(employee_data)\n",
    "\n",
    "# Add realistic constraints\n",
    "df['age'] = np.clip(df['age'], 22, 65)\n",
    "df['years_experience'] = np.clip(df['years_experience'], 0, df['age'] - 22)\n",
    "df['performance_score'] = np.clip(df['performance_score'], 1, 10)\n",
    "df['hours_per_week'] = np.clip(df['hours_per_week'], 20, 60)\n",
    "\n",
    "# Create salary based on realistic factors (this will be our target variable)\n",
    "base_salary = 50000\n",
    "education_bonus = {'High School': 0, 'Bachelor': 15000, 'Master': 25000, 'PhD': 40000}\n",
    "level_bonus = {'Junior': 0, 'Mid': 20000, 'Senior': 40000, 'Lead': 70000}\n",
    "dept_bonus = {'Engineering': 15000, 'Sales': 10000, 'Marketing': 5000, 'HR': 0, 'Finance': 8000}\n",
    "location_bonus = {'New York': 20000, 'San Francisco': 25000, 'Chicago': 5000, 'Austin': 8000, 'Remote': 0}\n",
    "\n",
    "df['salary'] = (base_salary + \n",
    "                df['education_level'].map(education_bonus) +\n",
    "                df['job_level'].map(level_bonus) +\n",
    "                df['department'].map(dept_bonus) +\n",
    "                df['location'].map(location_bonus) +\n",
    "                df['years_experience'] * 2000 +\n",
    "                df['performance_score'] * 3000 +\n",
    "                np.random.normal(0, 10000, n_employees))\n",
    "\n",
    "df['salary'] = np.maximum(df['salary'], 35000)  # Minimum salary\n",
    "\n",
    "print(f\"Dataset created with {len(df)} employees\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Realistic Data Quality Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce missing values (realistic patterns)\n",
    "# Performance scores might be missing for new employees\n",
    "new_employee_mask = df['years_experience'] < 0.5\n",
    "df.loc[new_employee_mask & (np.random.random(len(df)) < 0.3), 'performance_score'] = np.nan\n",
    "\n",
    "# Training hours might be missing randomly\n",
    "missing_training = np.random.choice(df.index, size=80, replace=False)\n",
    "df.loc[missing_training, 'training_hours'] = np.nan\n",
    "\n",
    "# Some education levels might be missing\n",
    "missing_education = np.random.choice(df.index, size=30, replace=False)\n",
    "df.loc[missing_education, 'education_level'] = np.nan\n",
    "\n",
    "# Add some outliers\n",
    "# Extremely high performers\n",
    "outlier_indices = np.random.choice(df.index, size=10, replace=False)\n",
    "df.loc[outlier_indices, 'hours_per_week'] = np.random.uniform(70, 80, 10)\n",
    "df.loc[outlier_indices, 'projects_completed'] = np.random.uniform(25, 35, 10)\n",
    "\n",
    "# Add some inconsistent data\n",
    "# Some employees with PhD but very low experience (career changers)\n",
    "career_changer_indices = np.random.choice(df[df['education_level'] == 'PhD'].index, size=5, replace=False)\n",
    "df.loc[career_changer_indices, 'years_experience'] = np.random.uniform(0, 2, 5)\n",
    "\n",
    "# Add some duplicate-like entries (same person, different records)\n",
    "duplicate_base = df.sample(3).copy()\n",
    "duplicate_base['employee_id'] = range(n_employees + 1, n_employees + 4)\n",
    "# Slightly modify some values to simulate data entry errors\n",
    "duplicate_base['age'] += np.random.randint(-1, 2, 3)\n",
    "duplicate_base['salary'] += np.random.randint(-5000, 5000, 3)\n",
    "df = pd.concat([df, duplicate_base], ignore_index=True)\n",
    "\n",
    "print(\"Data quality issues introduced:\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(\"\\nMissing values by column:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Quality Assessment\n",
    "Before preprocessing, let's understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality report\n",
    "def data_quality_report(df):\n",
    "    print(\"=== DATA QUALITY REPORT ===\")\n",
    "    print(f\"\\nðŸ“Š Dataset Overview:\")\n",
    "    print(f\"   â€¢ Shape: {df.shape}\")\n",
    "    print(f\"   â€¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "    \n",
    "    print(f\"\\nðŸ” Missing Values:\")\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    \n",
    "    for col in missing_data[missing_data > 0].index:\n",
    "        print(f\"   â€¢ {col}: {missing_data[col]} ({missing_percent[col]:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Data Types:\")\n",
    "    print(f\"   â€¢ Numerical columns: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "    print(f\"   â€¢ Categorical columns: {len(df.select_dtypes(include=['object']).columns)}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Potential Issues:\")\n",
    "    # Check for duplicates\n",
    "    duplicates = len(df) - len(df.drop_duplicates())\n",
    "    if duplicates > 0:\n",
    "        print(f\"   â€¢ Duplicate rows: {duplicates}\")\n",
    "    \n",
    "    # Check for outliers in numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numerical_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = len(df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)])\n",
    "        if outliers > 0:\n",
    "            print(f\"   â€¢ {col} outliers: {outliers} ({outliers/len(df)*100:.1f}%)\")\n",
    "\n",
    "data_quality_report(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Missing data heatmap\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Data Pattern')\n",
    "\n",
    "# Missing data bar plot\n",
    "plt.subplot(2, 2, 2)\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "missing_counts.plot(kind='bar')\n",
    "plt.title('Missing Values by Column')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Distribution of numerical variables with missing values\n",
    "plt.subplot(2, 2, 3)\n",
    "df['performance_score'].hist(bins=20, alpha=0.7)\n",
    "plt.title('Performance Score Distribution')\n",
    "plt.xlabel('Performance Score')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "df['training_hours'].hist(bins=20, alpha=0.7)\n",
    "plt.title('Training Hours Distribution')\n",
    "plt.xlabel('Training Hours')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Handling Missing Values\n",
    "Different strategies work better for different types of missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Understanding Missing Data Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing data patterns\n",
    "print(\"Missing Data Analysis:\")\n",
    "print(\"\\n1. Performance Score Missing Pattern:\")\n",
    "missing_perf = df[df['performance_score'].isnull()]\n",
    "print(f\"   â€¢ Average years of experience: {missing_perf['years_experience'].mean():.2f}\")\n",
    "print(f\"   â€¢ Most common job level: {missing_perf['job_level'].mode().iloc[0]}\")\n",
    "\n",
    "print(\"\\n2. Training Hours Missing Pattern:\")\n",
    "missing_training = df[df['training_hours'].isnull()]\n",
    "print(f\"   â€¢ Average age: {missing_training['age'].mean():.2f}\")\n",
    "print(f\"   â€¢ Department distribution:\")\n",
    "print(missing_training['department'].value_counts())\n",
    "\n",
    "print(\"\\n3. Education Level Missing Pattern:\")\n",
    "missing_edu = df[df['education_level'].isnull()]\n",
    "print(f\"   â€¢ Average salary: ${missing_edu['salary'].mean():.0f}\")\n",
    "print(f\"   â€¢ Average years experience: {missing_edu['years_experience'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Imputation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(\"Applying different imputation strategies...\")\n",
    "\n",
    "# Strategy 1: Mean imputation for performance_score (numerical)\n",
    "# But let's be smarter - use group mean based on job level\n",
    "performance_means = df_processed.groupby('job_level')['performance_score'].mean()\n",
    "print(\"\\nPerformance score means by job level:\")\n",
    "print(performance_means)\n",
    "\n",
    "for level in performance_means.index:\n",
    "    mask = (df_processed['job_level'] == level) & (df_processed['performance_score'].isnull())\n",
    "    df_processed.loc[mask, 'performance_score'] = performance_means[level]\n",
    "\n",
    "# Strategy 2: Median imputation for training_hours (skewed distribution)\n",
    "training_median = df_processed['training_hours'].median()\n",
    "df_processed['training_hours'].fillna(training_median, inplace=True)\n",
    "print(f\"\\nFilled training hours with median: {training_median:.1f}\")\n",
    "\n",
    "# Strategy 3: Mode imputation for education_level (categorical)\n",
    "education_mode = df_processed['education_level'].mode().iloc[0]\n",
    "df_processed['education_level'].fillna(education_mode, inplace=True)\n",
    "print(f\"Filled education level with mode: {education_mode}\")\n",
    "\n",
    "print(f\"\\nMissing values after imputation: {df_processed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions before and after imputation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Performance score\n",
    "axes[0, 0].hist(df['performance_score'].dropna(), bins=20, alpha=0.7, label='Original', density=True)\n",
    "axes[0, 0].hist(df_processed['performance_score'], bins=20, alpha=0.7, label='After Imputation', density=True)\n",
    "axes[0, 0].set_title('Performance Score Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Training hours\n",
    "axes[0, 1].hist(df['training_hours'].dropna(), bins=20, alpha=0.7, label='Original', density=True)\n",
    "axes[0, 1].hist(df_processed['training_hours'], bins=20, alpha=0.7, label='After Imputation', density=True)\n",
    "axes[0, 1].set_title('Training Hours Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Education level\n",
    "original_edu = df['education_level'].value_counts(normalize=True)\n",
    "imputed_edu = df_processed['education_level'].value_counts(normalize=True)\n",
    "axes[1, 0].bar(range(len(original_edu)), original_edu.values, alpha=0.7, label='Original')\n",
    "axes[1, 0].bar(range(len(imputed_edu)), imputed_edu.values, alpha=0.7, label='After Imputation')\n",
    "axes[1, 0].set_xticks(range(len(original_edu)))\n",
    "axes[1, 0].set_xticklabels(original_edu.index, rotation=45)\n",
    "axes[1, 0].set_title('Education Level Distribution')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Summary\n",
    "axes[1, 1].text(0.1, 0.8, 'Imputation Summary:', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].text(0.1, 0.6, f'â€¢ Performance: Group mean by job level', fontsize=12)\n",
    "axes[1, 1].text(0.1, 0.5, f'â€¢ Training hours: Median ({training_median:.1f})', fontsize=12)\n",
    "axes[1, 1].text(0.1, 0.4, f'â€¢ Education: Mode ({education_mode})', fontsize=12)\n",
    "axes[1, 1].text(0.1, 0.2, f'â€¢ Total missing values removed: {df.isnull().sum().sum()}', fontsize=12)\n",
    "axes[1, 1].set_xlim(0, 1)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Encoding Categorical Variables\n",
    "Machine learning algorithms work with numbers, so we need to convert categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_columns = ['education_level', 'department', 'job_level', 'location']\n",
    "print(\"Categorical columns to encode:\")\n",
    "for col in categorical_columns:\n",
    "    print(f\"â€¢ {col}: {df_processed[col].nunique()} unique values\")\n",
    "    print(f\"  Values: {list(df_processed[col].unique())}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Ordinal Encoding (for ordered categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education level has a natural order\n",
    "education_order = ['High School', 'Bachelor', 'Master', 'PhD']\n",
    "education_mapping = {level: i for i, level in enumerate(education_order)}\n",
    "df_processed['education_level_encoded'] = df_processed['education_level'].map(education_mapping)\n",
    "\n",
    "# Job level also has a natural order\n",
    "job_order = ['Junior', 'Mid', 'Senior', 'Lead']\n",
    "job_mapping = {level: i for i, level in enumerate(job_order)}\n",
    "df_processed['job_level_encoded'] = df_processed['job_level'].map(job_mapping)\n",
    "\n",
    "print(\"Ordinal Encoding Applied:\")\n",
    "print(\"\\nEducation Level Mapping:\")\n",
    "for original, encoded in education_mapping.items():\n",
    "    print(f\"  {original} â†’ {encoded}\")\n",
    "\n",
    "print(\"\\nJob Level Mapping:\")\n",
    "for original, encoded in job_mapping.items():\n",
    "    print(f\"  {original} â†’ {encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 One-Hot Encoding (for nominal categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Department and location don't have natural order - use one-hot encoding\n",
    "# Create dummy variables\n",
    "department_dummies = pd.get_dummies(df_processed['department'], prefix='dept')\n",
    "location_dummies = pd.get_dummies(df_processed['location'], prefix='loc')\n",
    "\n",
    "print(\"One-Hot Encoding Applied:\")\n",
    "print(f\"\\nDepartment columns created: {list(department_dummies.columns)}\")\n",
    "print(f\"Location columns created: {list(location_dummies.columns)}\")\n",
    "\n",
    "# Add to dataframe\n",
    "df_processed = pd.concat([df_processed, department_dummies, location_dummies], axis=1)\n",
    "\n",
    "print(f\"\\nDataset shape after encoding: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the encoding results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Education level encoding\n",
    "education_comparison = pd.DataFrame({\n",
    "    'Original': df_processed['education_level'].value_counts(),\n",
    "    'Encoded': df_processed.groupby('education_level')['education_level_encoded'].first()\n",
    "})\n",
    "axes[0, 0].bar(education_comparison.index, education_comparison['Encoded'])\n",
    "axes[0, 0].set_title('Education Level Ordinal Encoding')\n",
    "axes[0, 0].set_ylabel('Encoded Value')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Job level encoding\n",
    "job_comparison = pd.DataFrame({\n",
    "    'Original': df_processed['job_level'].value_counts(),\n",
    "    'Encoded': df_processed.groupby('job_level')['job_level_encoded'].first()\n",
    "})\n",
    "axes[0, 1].bar(job_comparison.index, job_comparison['Encoded'])\n",
    "axes[0, 1].set_title('Job Level Ordinal Encoding')\n",
    "axes[0, 1].set_ylabel('Encoded Value')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Department one-hot encoding\n",
    "dept_cols = [col for col in df_processed.columns if col.startswith('dept_')]\n",
    "dept_sums = df_processed[dept_cols].sum()\n",
    "axes[1, 0].bar(range(len(dept_sums)), dept_sums.values)\n",
    "axes[1, 0].set_xticks(range(len(dept_sums)))\n",
    "axes[1, 0].set_xticklabels([col.replace('dept_', '') for col in dept_sums.index], rotation=45)\n",
    "axes[1, 0].set_title('Department One-Hot Encoding')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# Location one-hot encoding\n",
    "loc_cols = [col for col in df_processed.columns if col.startswith('loc_')]\n",
    "loc_sums = df_processed[loc_cols].sum()\n",
    "axes[1, 1].bar(range(len(loc_sums)), loc_sums.values)\n",
    "axes[1, 1].set_xticks(range(len(loc_sums)))\n",
    "axes[1, 1].set_xticklabels([col.replace('loc_', '') for col in loc_sums.index], rotation=45)\n",
    "axes[1, 1].set_title('Location One-Hot Encoding')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Scaling and Normalization\n",
    "Different features have different scales, which can bias machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns for scaling\n",
    "numerical_columns = ['age', 'years_experience', 'performance_score', 'hours_per_week', \n",
    "                    'projects_completed', 'training_hours', 'salary']\n",
    "\n",
    "print(\"Numerical columns statistics before scaling:\")\n",
    "print(df_processed[numerical_columns].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the scale differences\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    axes[i].hist(df_processed[col], bins=30, alpha=0.7)\n",
    "    axes[i].set_title(f'{col}\\nRange: {df_processed[col].min():.0f} - {df_processed[col].max():.0f}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[7])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Standard Scaling (Z-score normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler (mean=0, std=1)\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "# We'll exclude salary from scaling since it's our target variable\n",
    "features_to_scale = [col for col in numerical_columns if col != 'salary']\n",
    "\n",
    "# Fit and transform\n",
    "df_standard_scaled = df_processed.copy()\n",
    "df_standard_scaled[features_to_scale] = scaler_standard.fit_transform(df_processed[features_to_scale])\n",
    "\n",
    "print(\"Standard Scaling Applied:\")\n",
    "print(\"\\nFeatures scaled (mean=0, std=1):\")\n",
    "print(df_standard_scaled[features_to_scale].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Min-Max Scaling (0-1 normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler (range 0-1)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "# Fit and transform\n",
    "df_minmax_scaled = df_processed.copy()\n",
    "df_minmax_scaled[features_to_scale] = scaler_minmax.fit_transform(df_processed[features_to_scale])\n",
    "\n",
    "print(\"Min-Max Scaling Applied:\")\n",
    "print(\"\\nFeatures scaled (range 0-1):\")\n",
    "print(df_minmax_scaled[features_to_scale].describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scaling methods\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "# Select a few key features for comparison\n",
    "comparison_features = ['age', 'years_experience', 'salary']\n",
    "\n",
    "for i, feature in enumerate(comparison_features):\n",
    "    # Original\n",
    "    axes[0, i].hist(df_processed[feature], bins=30, alpha=0.7, color='blue')\n",
    "    axes[0, i].set_title(f'Original {feature}')\n",
    "    axes[0, i].set_ylabel('Frequency')\n",
    "    \n",
    "    if feature != 'salary':  # Don't scale target variable\n",
    "        # Standard scaled\n",
    "        axes[1, i].hist(df_standard_scaled[feature], bins=30, alpha=0.7, color='green')\n",
    "        axes[1, i].set_title(f'Standard Scaled {feature}')\n",
    "        axes[1, i].set_ylabel('Frequency')\n",
    "        \n",
    "        # Min-max scaled\n",
    "        axes[2, i].hist(df_minmax_scaled[feature], bins=30, alpha=0.7, color='red')\n",
    "        axes[2, i].set_title(f'Min-Max Scaled {feature}')\n",
    "        axes[2, i].set_ylabel('Frequency')\n",
    "    else:\n",
    "        # For salary, show the same distribution\n",
    "        axes[1, i].hist(df_processed[feature], bins=30, alpha=0.7, color='blue')\n",
    "        axes[1, i].set_title(f'{feature} (Target - Not Scaled)')\n",
    "        axes[1, i].set_ylabel('Frequency')\n",
    "        \n",
    "        axes[2, i].hist(df_processed[feature], bins=30, alpha=0.7, color='blue')\n",
    "        axes[2, i].set_title(f'{feature} (Target - Not Scaled)')\n",
    "        axes[2, i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Outlier Detection and Treatment\n",
    "Outliers can significantly impact model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method\n",
    "def detect_outliers_iqr(df, column, multiplier=1.5):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Detect outliers in key numerical columns\n",
    "outlier_columns = ['hours_per_week', 'projects_completed', 'salary']\n",
    "\n",
    "print(\"Outlier Detection Results:\")\n",
    "for col in outlier_columns:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_processed, col)\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Normal range: {lower:.2f} to {upper:.2f}\")\n",
    "    print(f\"  Outliers found: {len(outliers)} ({len(outliers)/len(df_processed)*100:.1f}%)\")\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"  Outlier values: {sorted(outliers[col].values)[:5]}...\")  # Show first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(outlier_columns):\n",
    "    # Box plot\n",
    "    axes[i*2].boxplot(df_processed[col])\n",
    "    axes[i*2].set_title(f'{col} - Box Plot')\n",
    "    axes[i*2].set_ylabel(col)\n",
    "    \n",
    "    # Histogram with outlier boundaries\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_processed, col)\n",
    "    axes[i*2+1].hist(df_processed[col], bins=30, alpha=0.7)\n",
    "    axes[i*2+1].axvline(lower, color='red', linestyle='--', label=f'Lower bound: {lower:.1f}')\n",
    "    axes[i*2+1].axvline(upper, color='red', linestyle='--', label=f'Upper bound: {upper:.1f}')\n",
    "    axes[i*2+1].set_title(f'{col} - Distribution with Outlier Bounds')\n",
    "    axes[i*2+1].set_xlabel(col)\n",
    "    axes[i*2+1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers - we'll use capping (Winsorization)\n",
    "df_outlier_treated = df_processed.copy()\n",
    "\n",
    "for col in outlier_columns:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_processed, col)\n",
    "    \n",
    "    # Cap outliers at the bounds\n",
    "    df_outlier_treated[col] = df_outlier_treated[col].clip(lower=lower, upper=upper)\n",
    "    \n",
    "    print(f\"\\n{col} outlier treatment:\")\n",
    "    print(f\"  Values capped below {lower:.2f}: {len(df_processed[df_processed[col] < lower])}\")\n",
    "    print(f\"  Values capped above {upper:.2f}: {len(df_processed[df_processed[col] > upper])}\")\n",
    "\n",
    "print(\"\\nOutlier treatment completed using Winsorization (capping).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Feature Engineering\n",
    "Creating new features that might be more predictive than the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features based on domain knowledge\n",
    "df_engineered = df_outlier_treated.copy()\n",
    "\n",
    "print(\"Creating new features...\")\n",
    "\n",
    "# 1. Experience-to-age ratio (career focus indicator)\n",
    "df_engineered['experience_age_ratio'] = df_engineered['years_experience'] / df_engineered['age']\n",
    "\n",
    "# 2. Productivity score (projects per year of experience)\n",
    "df_engineered['productivity_score'] = df_engineered['projects_completed'] / (df_engineered['years_experience'] + 1)  # +1 to avoid division by zero\n",
    "\n",
    "# 3. Work intensity (hours per week relative to standard 40)\n",
    "df_engineered['work_intensity'] = df_engineered['hours_per_week'] / 40\n",
    "\n",
    "# 4. Training investment (training hours per year of experience)\n",
    "df_engineered['training_investment'] = df_engineered['training_hours'] / (df_engineered['years_experience'] + 1)\n",
    "\n",
    "# 5. Performance-experience interaction\n",
    "df_engineered['performance_experience'] = df_engineered['performance_score'] * df_engineered['years_experience']\n",
    "\n",
    "# 6. Age groups (categorical feature from numerical)\n",
    "df_engineered['age_group'] = pd.cut(df_engineered['age'], \n",
    "                                   bins=[0, 30, 40, 50, 100], \n",
    "                                   labels=['Young', 'Mid-Career', 'Experienced', 'Senior'])\n",
    "\n",
    "# 7. Experience level (categorical feature from numerical)\n",
    "df_engineered['experience_level'] = pd.cut(df_engineered['years_experience'], \n",
    "                                          bins=[0, 2, 5, 10, 100], \n",
    "                                          labels=['Novice', 'Intermediate', 'Experienced', 'Expert'])\n",
    "\n",
    "# 8. High performer flag\n",
    "df_engineered['high_performer'] = (df_engineered['performance_score'] > df_engineered['performance_score'].quantile(0.75)).astype(int)\n",
    "\n",
    "print(\"New features created:\")\n",
    "new_features = ['experience_age_ratio', 'productivity_score', 'work_intensity', \n",
    "                'training_investment', 'performance_experience', 'age_group', \n",
    "                'experience_level', 'high_performer']\n",
    "for feature in new_features:\n",
    "    print(f\"  â€¢ {feature}\")\n",
    "\n",
    "print(f\"\\nDataset shape after feature engineering: {df_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the new features\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "numerical_new_features = ['experience_age_ratio', 'productivity_score', 'work_intensity', \n",
    "                         'training_investment', 'performance_experience']\n",
    "\n",
    "for i, feature in enumerate(numerical_new_features):\n",
    "    axes[i].hist(df_engineered[feature], bins=30, alpha=0.7)\n",
    "    axes[i].set_title(f'{feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Age group distribution\n",
    "age_group_counts = df_engineered['age_group'].value_counts()\n",
    "axes[5].bar(age_group_counts.index, age_group_counts.values)\n",
    "axes[5].set_title('Age Group Distribution')\n",
    "axes[5].set_ylabel('Count')\n",
    "\n",
    "# Experience level distribution\n",
    "exp_level_counts = df_engineered['experience_level'].value_counts()\n",
    "axes[6].bar(exp_level_counts.index, exp_level_counts.values)\n",
    "axes[6].set_title('Experience Level Distribution')\n",
    "axes[6].set_ylabel('Count')\n",
    "\n",
    "# High performer distribution\n",
    "high_perf_counts = df_engineered['high_performer'].value_counts()\n",
    "axes[7].bar(['Regular', 'High Performer'], high_perf_counts.values)\n",
    "axes[7].set_title('High Performer Distribution')\n",
    "axes[7].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Selection\n",
    "Not all features are equally important. Let's identify the most predictive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for feature importance analysis\n",
    "# Select numerical features for correlation analysis\n",
    "numerical_features_all = df_engineered.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove ID and target variable\n",
    "numerical_features_all = [col for col in numerical_features_all if col not in ['employee_id', 'salary']]\n",
    "\n",
    "# Calculate correlation with target variable (salary)\n",
    "correlations = df_engineered[numerical_features_all + ['salary']].corr()['salary'].abs().sort_values(ascending=False)\n",
    "correlations = correlations.drop('salary')  # Remove self-correlation\n",
    "\n",
    "print(\"Feature Correlation with Salary (absolute values):\")\n",
    "print(correlations.round(3))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = correlations.head(10)\n",
    "plt.barh(range(len(top_features)), top_features.values)\n",
    "plt.yticks(range(len(top_features)), top_features.index)\n",
    "plt.xlabel('Absolute Correlation with Salary')\n",
    "plt.title('Top 10 Features by Correlation with Salary')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Random Forest for feature importance\n",
    "# Prepare features (only numerical for this example)\n",
    "X = df_engineered[numerical_features_all]\n",
    "y = df_engineered['salary']\n",
    "\n",
    "# Train a Random Forest to get feature importance\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nRandom Forest Feature Importance:\")\n",
    "print(feature_importance.round(4))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_rf_features = feature_importance.head(10)\n",
    "plt.barh(range(len(top_rf_features)), top_rf_features['importance'])\n",
    "plt.yticks(range(len(top_rf_features)), top_rf_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 Features by Random Forest Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Building a Preprocessing Pipeline\n",
    "Let's create a reusable pipeline for all our preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive preprocessing pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define column groups\n",
    "numerical_cols = ['age', 'years_experience', 'performance_score', 'hours_per_week', \n",
    "                 'projects_completed', 'training_hours']\n",
    "categorical_cols = ['education_level', 'department', 'job_level', 'location']\n",
    "\n",
    "# Create preprocessing pipelines for different column types\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "print(\"Preprocessing Pipeline Created:\")\n",
    "print(\"\\nNumerical Pipeline:\")\n",
    "print(\"  1. Impute missing values with median\")\n",
    "print(\"  2. Standard scaling (mean=0, std=1)\")\n",
    "print(\"\\nCategorical Pipeline:\")\n",
    "print(\"  1. Impute missing values with mode\")\n",
    "print(\"  2. One-hot encoding (drop first category)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline on our data\n",
    "# Start with original data (with missing values)\n",
    "X_original = df[numerical_cols + categorical_cols]\n",
    "y_original = df['salary']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_original, y_original, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"\\nMissing values in training set: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test set: {X_test.isnull().sum().sum()}\")\n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\nAfter preprocessing:\")\n",
    "print(f\"Training set shape: {X_train_processed.shape}\")\n",
    "print(f\"Test set shape: {X_test_processed.shape}\")\n",
    "print(f\"Missing values in processed training set: {np.isnan(X_train_processed).sum()}\")\n",
    "print(f\"Missing values in processed test set: {np.isnan(X_test_processed).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "# Numerical features keep their names\n",
    "num_feature_names = numerical_cols\n",
    "\n",
    "# Categorical features get expanded\n",
    "cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "\n",
    "all_feature_names = num_feature_names + list(cat_feature_names)\n",
    "\n",
    "print(f\"Total features after preprocessing: {len(all_feature_names)}\")\n",
    "print(\"\\nFeature names:\")\n",
    "for i, name in enumerate(all_feature_names):\n",
    "    print(f\"  {i+1:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Your Turn to Practice!\n",
    "Now it's your turn to apply preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Create a new feature\n",
    "Create a feature that represents \"career progression speed\" (job level encoded divided by years of experience). Handle the case where years of experience is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 1\n",
    "# Hint: Use the job_level_encoded and years_experience columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Handle a new categorical variable\n",
    "Imagine we have a new column 'work_style' with values ['Remote', 'Hybrid', 'Office']. Add appropriate preprocessing for this column to our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 2\n",
    "# Hint: You'll need to modify the categorical_cols list and recreate the preprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Compare imputation strategies\n",
    "Compare the effect of using mean vs median imputation for the 'performance_score' column. Which one preserves the distribution better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 3\n",
    "# Hint: Create two versions of the data with different imputation strategies and compare histograms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've mastered the essential data preprocessing and feature engineering techniques. Here's what you've learned:\n",
    "\n",
    "### âœ… Key Skills Mastered:\n",
    "1. **Data Quality Assessment**: Identifying and understanding missing values, duplicates, and outliers\n",
    "2. **Missing Value Handling**: Multiple imputation strategies (mean, median, mode, group-based)\n",
    "3. **Categorical Encoding**: Ordinal encoding for ordered categories, one-hot encoding for nominal categories\n",
    "4. **Feature Scaling**: StandardScaler and MinMaxScaler for different use cases\n",
    "5. **Outlier Treatment**: Detection using IQR method and treatment using Winsorization\n",
    "6. **Feature Engineering**: Creating new features from existing ones using domain knowledge\n",
    "7. **Feature Selection**: Using correlation and Random Forest importance for feature ranking\n",
    "8. **Pipeline Creation**: Building reusable preprocessing pipelines with scikit-learn\n",
    "\n",
    "### ðŸ”§ Best Practices Learned:\n",
    "- Always understand your data before preprocessing\n",
    "- Choose imputation strategies based on data distribution and missing patterns\n",
    "- Use appropriate encoding for different types of categorical variables\n",
    "- Scale features when algorithms are sensitive to feature magnitude\n",
    "- Handle outliers based on domain knowledge and model requirements\n",
    "- Create features that capture domain expertise\n",
    "- Use pipelines to ensure reproducibility and prevent data leakage\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "In the next lab, we'll use this preprocessed data to build and evaluate classification models, learning how to:\n",
    "- Train logistic regression and decision tree classifiers\n",
    "- Evaluate model performance using various metrics\n",
    "- Use cross-validation for robust model assessment\n",
    "- Interpret model results and make predictions\n",
    "\n",
    "### ðŸ“š Additional Resources:\n",
    "- [Scikit-learn Preprocessing Guide](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- [Feature Engineering Techniques](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114)\n",
    "- [Handling Missing Data](https://towardsdatascience.com/handling-missing-values-in-machine-learning-part-1-dda70d4dd536)\n",
    "- [Pipeline Documentation](https://scikit-learn.org/stable/modules/compose.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

