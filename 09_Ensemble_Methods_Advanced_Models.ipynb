{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods and Advanced Models\n",
    "## Module 8, Lab 5: Combining Models for Better Performance\n",
    "\n",
    "Individual models have limitations, but ensemble methods combine multiple models to achieve better performance, reduce overfitting, and increase robustness. This lab explores the most powerful ensemble techniques used in machine learning.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand the principles behind ensemble methods\n",
    "- Build Random Forest models (bagging)\n",
    "- Implement Gradient Boosting models (boosting)\n",
    "- Create voting and stacking ensembles\n",
    "- Tune hyperparameters for optimal performance\n",
    "- Compare ensemble methods with individual models\n",
    "\n",
    "### Why Ensemble Methods Matter\n",
    "Ensemble methods often win machine learning competitions and are widely used in industry because they:\n",
    "- Reduce overfitting through model averaging\n",
    "- Improve generalization to new data\n",
    "- Provide more robust predictions\n",
    "- Can capture different patterns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, \n",
    "    VotingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Complex Dataset\n",
    "We'll create a more complex dataset that benefits from ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complex customer churn dataset\n",
    "np.random.seed(42)\n",
    "n_customers = 2000\n",
    "\n",
    "# Generate customer features with complex interactions\n",
    "customer_data = {\n",
    "    'age': np.random.normal(40, 15, n_customers),\n",
    "    'income': np.random.lognormal(10.5, 0.6, n_customers),\n",
    "    'account_length': np.random.exponential(3, n_customers),\n",
    "    'monthly_charges': np.random.normal(65, 20, n_customers),\n",
    "    'total_charges': np.random.normal(1500, 800, n_customers),\n",
    "    'support_calls': np.random.poisson(2, n_customers),\n",
    "    'contract_length': np.random.choice([1, 12, 24], n_customers, p=[0.4, 0.35, 0.25]),\n",
    "    'payment_method': np.random.choice(['Credit Card', 'Bank Transfer', 'Electronic Check', 'Mailed Check'], \n",
    "                                      n_customers, p=[0.35, 0.25, 0.25, 0.15]),\n",
    "    'internet_service': np.random.choice(['DSL', 'Fiber', 'No'], n_customers, p=[0.4, 0.45, 0.15]),\n",
    "    'online_security': np.random.choice([0, 1], n_customers, p=[0.6, 0.4]),\n",
    "    'tech_support': np.random.choice([0, 1], n_customers, p=[0.7, 0.3]),\n",
    "    'streaming_tv': np.random.choice([0, 1], n_customers, p=[0.55, 0.45]),\n",
    "    'streaming_movies': np.random.choice([0, 1], n_customers, p=[0.55, 0.45]),\n",
    "    'paperless_billing': np.random.choice([0, 1], n_customers, p=[0.4, 0.6]),\n",
    "    'senior_citizen': np.random.choice([0, 1], n_customers, p=[0.84, 0.16])\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(customer_data)\n",
    "\n",
    "# Apply realistic constraints\n",
    "df['age'] = np.clip(df['age'], 18, 80)\n",
    "df['income'] = np.clip(df['income'], 20000, 200000)\n",
    "df['account_length'] = np.clip(df['account_length'], 0, 10)\n",
    "df['monthly_charges'] = np.clip(df['monthly_charges'], 20, 120)\n",
    "df['total_charges'] = np.maximum(df['total_charges'], df['monthly_charges'] * df['account_length'])\n",
    "\n",
    "# Create complex feature interactions\n",
    "df['charges_to_income_ratio'] = df['monthly_charges'] / (df['income'] / 12)\n",
    "df['avg_monthly_charges'] = df['total_charges'] / (df['account_length'] + 1)\n",
    "df['service_count'] = (df['online_security'] + df['tech_support'] + \n",
    "                      df['streaming_tv'] + df['streaming_movies'])\n",
    "\n",
    "print(f\"Dataset created with {len(df)} customers\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Target Variable (Churn)\n",
    "We'll create a complex churn pattern that benefits from ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complex churn probability with non-linear relationships\n",
    "churn_probability = (\n",
    "    0.05 +  # Base probability\n",
    "    # Linear effects\n",
    "    (df['support_calls'] / 10) * 0.3 +\n",
    "    (df['charges_to_income_ratio'] > 0.15) * 0.2 +\n",
    "    (df['contract_length'] == 1) * 0.25 +\n",
    "    (df['payment_method'] == 'Electronic Check') * 0.15 +\n",
    "    (df['paperless_billing'] == 1) * 0.1 +\n",
    "    (df['senior_citizen'] == 1) * 0.1 +\n",
    "    \n",
    "    # Non-linear effects (quadratic)\n",
    "    ((df['age'] - 40) ** 2 / 1000) * 0.1 +\n",
    "    \n",
    "    # Interaction effects\n",
    "    (df['internet_service'] == 'Fiber') * (df['tech_support'] == 0) * 0.2 +\n",
    "    (df['account_length'] < 1) * (df['monthly_charges'] > 80) * 0.3 +\n",
    "    (df['service_count'] == 0) * 0.15 +\n",
    "    \n",
    "    # Random noise\n",
    "    np.random.normal(0, 0.05, n_customers)\n",
    ")\n",
    "\n",
    "# Ensure probabilities are between 0 and 1\n",
    "churn_probability = np.clip(churn_probability, 0, 0.8)\n",
    "\n",
    "# Generate binary churn outcome\n",
    "df['churn'] = np.random.binomial(1, churn_probability)\n",
    "\n",
    "print(f\"Churn rate: {df['churn'].mean():.2%}\")\n",
    "print(f\"Customers who churned: {df['churn'].sum()}\")\n",
    "print(f\"Customers who stayed: {len(df) - df['churn'].sum()}\")\n",
    "\n",
    "# Display churn by key features\n",
    "print(\"\\nChurn rates by key features:\")\n",
    "print(f\"Contract length: {df.groupby('contract_length')['churn'].mean().round(3)}\")\n",
    "print(f\"Payment method: {df.groupby('payment_method')['churn'].mean().round(3)}\")\n",
    "print(f\"Internet service: {df.groupby('internet_service')['churn'].mean().round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation\n",
    "Let's prepare our data for ensemble modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "numerical_features = ['age', 'income', 'account_length', 'monthly_charges', 'total_charges', \n",
    "                     'support_calls', 'charges_to_income_ratio', 'avg_monthly_charges', 'service_count']\n",
    "categorical_features = ['contract_length', 'payment_method', 'internet_service']\n",
    "binary_features = ['online_security', 'tech_support', 'streaming_tv', 'streaming_movies', \n",
    "                  'paperless_billing', 'senior_citizen']\n",
    "\n",
    "# Combine all features\n",
    "all_features = numerical_features + categorical_features + binary_features\n",
    "X = df[all_features]\n",
    "y = df['churn']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature types:\")\n",
    "print(f\"  Numerical: {len(numerical_features)}\")\n",
    "print(f\"  Categorical: {len(categorical_features)}\")\n",
    "print(f\"  Binary: {len(binary_features)}\")\n",
    "print(f\"\\nClass distribution: {y.value_counts(normalize=True).round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features),\n",
    "        ('bin', 'passthrough', binary_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\nProcessed training set shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed test set shape: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Individual Base Models\n",
    "Let's first establish baseline performance with individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models\n",
    "base_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "    'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "# Train and evaluate base models\n",
    "base_results = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Training and evaluating base models...\")\n",
    "for name, model in base_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Train on full training set\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "    \n",
    "    # Store results\n",
    "    base_results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred),\n",
    "        'test_precision': precision_score(y_test, y_pred),\n",
    "        'test_recall': recall_score(y_test, y_pred),\n",
    "        'test_f1': f1_score(y_test, y_pred),\n",
    "        'test_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"  Test AUC: {base_results[name]['test_auc']:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    name: {\n",
    "        'CV_AUC': results['cv_mean'],\n",
    "        'Test_Accuracy': results['test_accuracy'],\n",
    "        'Test_Precision': results['test_precision'],\n",
    "        'Test_Recall': results['test_recall'],\n",
    "        'Test_F1': results['test_f1'],\n",
    "        'Test_AUC': results['test_auc']\n",
    "    }\n",
    "    for name, results in base_results.items()\n",
    "}).T\n",
    "\n",
    "print(\"\\nBase Models Performance:\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Bagging Methods\n",
    "Bagging (Bootstrap Aggregating) trains multiple models on different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest with different configurations\n",
    "rf_configs = {\n",
    "    'RF_Basic': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'RF_Tuned': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        max_features='sqrt',\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "rf_results = {}\n",
    "\n",
    "print(\"Training Random Forest models...\")\n",
    "for name, model in rf_configs.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Train on full training set\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "    \n",
    "    # Store results\n",
    "    rf_results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'test_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'test_f1': f1_score(y_test, y_pred),\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"  CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"  Test AUC: {rf_results[name]['test_auc']:.4f}\")\n",
    "    print(f\"  Test F1: {rf_results[name]['test_f1']:.4f}\")\n",
    "\n",
    "# Get the best Random Forest model\n",
    "best_rf_name = max(rf_results.keys(), key=lambda x: rf_results[x]['test_auc'])\n",
    "best_rf_model = rf_results[best_rf_name]['model']\n",
    "print(f\"\\nBest Random Forest: {best_rf_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Random Forest feature importance\n",
    "# Get feature names after preprocessing\n",
    "num_feature_names = numerical_features\n",
    "cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "bin_feature_names = binary_features\n",
    "all_feature_names = num_feature_names + list(cat_feature_names) + bin_feature_names\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'importance': best_rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (Random Forest):\")\n",
    "print(feature_importance.head(10).round(4))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importances in Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bagging with Different Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try bagging with different base estimators\n",
    "bagging_models = {\n",
    "    'Bagging_DT': BaggingClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(max_depth=10),\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Bagging_LR': BaggingClassifier(\n",
    "        base_estimator=LogisticRegression(max_iter=1000),\n",
    "        n_estimators=50,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "bagging_results = {}\n",
    "\n",
    "print(\"Training Bagging models...\")\n",
    "for name, model in bagging_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "    \n",
    "    bagging_results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'test_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    print(f\"  CV AUC: {cv_scores.mean():.4f}\")\n",
    "    print(f\"  Test AUC: {bagging_results[name]['test_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Boosting Methods\n",
    "Boosting trains models sequentially, with each model learning from the mistakes of the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting models\n",
    "gb_configs = {\n",
    "    'GB_Basic': GradientBoostingClassifier(random_state=42),\n",
    "    'GB_Tuned': GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "gb_results = {}\n",
    "\n",
    "print(\"Training Gradient Boosting models...\")\n",
    "for name, model in gb_configs.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Train on full training set\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "    \n",
    "    # Store results\n",
    "    gb_results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'test_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'test_f1': f1_score(y_test, y_pred),\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"  CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"  Test AUC: {gb_results[name]['test_auc']:.4f}\")\n",
    "    print(f\"  Test F1: {gb_results[name]['test_f1']:.4f}\")\n",
    "\n",
    "# Get the best Gradient Boosting model\n",
    "best_gb_name = max(gb_results.keys(), key=lambda x: gb_results[x]['test_auc'])\n",
    "best_gb_model = gb_results[best_gb_name]['model']\n",
    "print(f\"\\nBest Gradient Boosting: {best_gb_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "print(\"Training XGBoost model...\")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(xgb_model, X_train_processed, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "# Train on full training set\n",
    "xgb_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test_processed)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "xgb_results = {\n",
    "    'cv_mean': cv_scores.mean(),\n",
    "    'cv_std': cv_scores.std(),\n",
    "    'test_auc': roc_auc_score(y_test, y_pred_proba_xgb),\n",
    "    'test_f1': f1_score(y_test, y_pred_xgb)\n",
    "}\n",
    "\n",
    "print(f\"XGBoost CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "print(f\"XGBoost Test AUC: {xgb_results['test_auc']:.4f}\")\n",
    "print(f\"XGBoost Test F1: {xgb_results['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AdaBoost model\n",
    "print(\"Training AdaBoost model...\")\n",
    "\n",
    "ada_model = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(ada_model, X_train_processed, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "# Train and evaluate\n",
    "ada_model.fit(X_train_processed, y_train)\n",
    "y_pred_ada = ada_model.predict(X_test_processed)\n",
    "y_pred_proba_ada = ada_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "ada_results = {\n",
    "    'cv_mean': cv_scores.mean(),\n",
    "    'test_auc': roc_auc_score(y_test, y_pred_proba_ada),\n",
    "    'test_f1': f1_score(y_test, y_pred_ada)\n",
    "}\n",
    "\n",
    "print(f\"AdaBoost CV AUC: {cv_scores.mean():.4f}\")\n",
    "print(f\"AdaBoost Test AUC: {ada_results['test_auc']:.4f}\")\n",
    "print(f\"AdaBoost Test F1: {ada_results['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Voting Ensembles\n",
    "Voting ensembles combine predictions from multiple different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create voting ensembles\n",
    "print(\"Creating Voting Ensembles...\")\n",
    "\n",
    "# Select best models from each category\n",
    "voting_estimators = [\n",
    "    ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "    ('rf', best_rf_model),\n",
    "    ('gb', best_gb_model),\n",
    "    ('xgb', xgb_model)\n",
    "]\n",
    "\n",
    "# Hard voting (majority vote)\n",
    "hard_voting = VotingClassifier(\n",
    "    estimators=voting_estimators,\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Soft voting (average probabilities)\n",
    "soft_voting = VotingClassifier(\n",
    "    estimators=voting_estimators,\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_models = {\n",
    "    'Hard_Voting': hard_voting,\n",
    "    'Soft_Voting': soft_voting\n",
    "}\n",
    "\n",
    "voting_results = {}\n",
    "\n",
    "for name, model in voting_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    \n",
    "    if name == 'Soft_Voting':\n",
    "        y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "    else:\n",
    "        # For hard voting, use the average of individual model probabilities\n",
    "        individual_probas = []\n",
    "        for est_name, estimator in voting_estimators:\n",
    "            individual_probas.append(estimator.predict_proba(X_test_processed)[:, 1])\n",
    "        y_pred_proba = np.mean(individual_probas, axis=0)\n",
    "    \n",
    "    voting_results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'test_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'test_f1': f1_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    print(f\"  CV AUC: {cv_scores.mean():.4f}\")\n",
    "    print(f\"  Test AUC: {voting_results[name]['test_auc']:.4f}\")\n",
    "    print(f\"  Test F1: {voting_results[name]['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Comparison and Analysis\n",
    "Let's compare all our ensemble methods with the base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = {}\n",
    "\n",
    "# Base models\n",
    "for name, results in base_results.items():\n",
    "    all_results[name] = {\n",
    "        'Type': 'Base Model',\n",
    "        'CV_AUC': results['cv_mean'],\n",
    "        'Test_AUC': results['test_auc'],\n",
    "        'Test_F1': results['test_f1']\n",
    "    }\n",
    "\n",
    "# Random Forest\n",
    "for name, results in rf_results.items():\n",
    "    all_results[name] = {\n",
    "        'Type': 'Bagging',\n",
    "        'CV_AUC': results['cv_mean'],\n",
    "        'Test_AUC': results['test_auc'],\n",
    "        'Test_F1': results['test_f1']\n",
    "    }\n",
    "\n",
    "# Gradient Boosting\n",
    "for name, results in gb_results.items():\n",
    "    all_results[name] = {\n",
    "        'Type': 'Boosting',\n",
    "        'CV_AUC': results['cv_mean'],\n",
    "        'Test_AUC': results['test_auc'],\n",
    "        'Test_F1': results['test_f1']\n",
    "    }\n",
    "\n",
    "# XGBoost\n",
    "all_results['XGBoost'] = {\n",
    "    'Type': 'Boosting',\n",
    "    'CV_AUC': xgb_results['cv_mean'],\n",
    "    'Test_AUC': xgb_results['test_auc'],\n",
    "    'Test_F1': xgb_results['test_f1']\n",
    "}\n",
    "\n",
    "# AdaBoost\n",
    "all_results['AdaBoost'] = {\n",
    "    'Type': 'Boosting',\n",
    "    'CV_AUC': ada_results['cv_mean'],\n",
    "    'Test_AUC': ada_results['test_auc'],\n",
    "    'Test_F1': ada_results['test_f1']\n",
    "}\n",
    "\n",
    "# Voting\n",
    "for name, results in voting_results.items():\n",
    "    all_results[name] = {\n",
    "        'Type': 'Voting',\n",
    "        'CV_AUC': results['cv_mean'],\n",
    "        'Test_AUC': results['test_auc'],\n",
    "        'Test_F1': results['test_f1']\n",
    "    }\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "final_results_df = pd.DataFrame(all_results).T\n",
    "final_results_df = final_results_df.sort_values('Test_AUC', ascending=False)\n",
    "\n",
    "print(\"=== COMPREHENSIVE MODEL COMPARISON ===\")\n",
    "print(final_results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# AUC comparison\n",
    "colors = {'Base Model': 'lightblue', 'Bagging': 'lightgreen', \n",
    "          'Boosting': 'lightcoral', 'Voting': 'lightyellow'}\n",
    "model_colors = [colors[model_type] for model_type in final_results_df['Type']]\n",
    "\n",
    "axes[0].barh(range(len(final_results_df)), final_results_df['Test_AUC'], color=model_colors)\n",
    "axes[0].set_yticks(range(len(final_results_df)))\n",
    "axes[0].set_yticklabels(final_results_df.index)\n",
    "axes[0].set_xlabel('Test AUC Score')\n",
    "axes[0].set_title('Model Performance Comparison (AUC)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(final_results_df['Test_AUC']):\n",
    "    axes[0].text(v + 0.005, i, f'{v:.3f}', va='center')\n",
    "\n",
    "# F1 comparison\n",
    "axes[1].barh(range(len(final_results_df)), final_results_df['Test_F1'], color=model_colors)\n",
    "axes[1].set_yticks(range(len(final_results_df)))\n",
    "axes[1].set_yticklabels(final_results_df.index)\n",
    "axes[1].set_xlabel('Test F1 Score')\n",
    "axes[1].set_title('Model Performance Comparison (F1)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(final_results_df['Test_F1']):\n",
    "    axes[1].text(v + 0.005, i, f'{v:.3f}', va='center')\n",
    "\n",
    "# Create legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, label=model_type) \n",
    "                  for model_type, color in colors.items()]\n",
    "fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.02), ncol=4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance improvement analysis\n",
    "best_base_model = final_results_df[final_results_df['Type'] == 'Base Model']['Test_AUC'].max()\n",
    "best_ensemble_model = final_results_df[final_results_df['Type'] != 'Base Model']['Test_AUC'].max()\n",
    "best_overall_model = final_results_df.iloc[0]\n",
    "\n",
    "improvement = best_ensemble_model - best_base_model\n",
    "improvement_pct = (improvement / best_base_model) * 100\n",
    "\n",
    "print(\"=== ENSEMBLE METHODS ANALYSIS ===\")\n",
    "print(f\"\\nüìä Performance Summary:\")\n",
    "print(f\"   ‚Ä¢ Best base model AUC: {best_base_model:.4f}\")\n",
    "print(f\"   ‚Ä¢ Best ensemble model AUC: {best_ensemble_model:.4f}\")\n",
    "print(f\"   ‚Ä¢ Improvement: {improvement:.4f} ({improvement_pct:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Best overall model: {best_overall_model.name} (AUC: {best_overall_model['Test_AUC']:.4f})\")\n",
    "\n",
    "print(f\"\\nüéØ Key Insights:\")\n",
    "print(f\"   ‚Ä¢ Ensemble methods {'improved' if improvement > 0 else 'did not improve'} upon base models\")\n",
    "\n",
    "# Analyze by ensemble type\n",
    "ensemble_performance = final_results_df[final_results_df['Type'] != 'Base Model'].groupby('Type')['Test_AUC'].agg(['mean', 'max', 'count'])\n",
    "print(f\"\\nüìà Ensemble Type Performance:\")\n",
    "for ensemble_type in ensemble_performance.index:\n",
    "    stats = ensemble_performance.loc[ensemble_type]\n",
    "    print(f\"   ‚Ä¢ {ensemble_type}: Avg AUC = {stats['mean']:.4f}, Best AUC = {stats['max']:.4f}, Models = {stats['count']}\")\n",
    "\n",
    "# Best model recommendations\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "top_3_models = final_results_df.head(3)\n",
    "print(f\"   ‚Ä¢ Top 3 models for deployment:\")\n",
    "for i, (model_name, model_data) in enumerate(top_3_models.iterrows(), 1):\n",
    "    print(f\"     {i}. {model_name} ({model_data['Type']}) - AUC: {model_data['Test_AUC']:.4f}\")\n",
    "\n",
    "print(f\"   ‚Ä¢ Consider ensemble diversity and computational cost\")\n",
    "print(f\"   ‚Ä¢ Soft voting often performs better than hard voting\")\n",
    "print(f\"   ‚Ä¢ Boosting methods (XGBoost, GB) often excel on tabular data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Hyperparameter Tuning\n",
    "Let's optimize the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for the best model (assuming it's XGBoost or Random Forest)\n",
    "best_model_name = final_results_df.index[0]\n",
    "print(f\"Performing hyperparameter tuning for: {best_model_name}\")\n",
    "\n",
    "if 'XGBoost' in best_model_name or 'GB' in best_model_name:\n",
    "    # Tune XGBoost/Gradient Boosting\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [4, 6, 8],\n",
    "        'subsample': [0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    if 'XGBoost' in best_model_name:\n",
    "        base_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    else:\n",
    "        base_model = GradientBoostingClassifier(random_state=42)\n",
    "        \n",
    "elif 'RF' in best_model_name:\n",
    "    # Tune Random Forest\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [5, 10, 20],\n",
    "        'min_samples_leaf': [2, 5, 10]\n",
    "    }\n",
    "    base_model = RandomForestClassifier(random_state=42)\n",
    "else:\n",
    "    # Default to Random Forest if unclear\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15],\n",
    "        'min_samples_split': [5, 10]\n",
    "    }\n",
    "    base_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Performing Grid Search (this may take a few minutes...)\")\n",
    "grid_search = GridSearchCV(\n",
    "    base_model,\n",
    "    param_grid,\n",
    "    cv=3,  # Reduced for speed\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_processed, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "tuned_model = grid_search.best_estimator_\n",
    "y_pred_tuned = tuned_model.predict(X_test_processed)\n",
    "y_pred_proba_tuned = tuned_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "tuned_auc = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "tuned_f1 = f1_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(f\"\\nTuned model performance:\")\n",
    "print(f\"  Test AUC: {tuned_auc:.4f}\")\n",
    "print(f\"  Test F1: {tuned_f1:.4f}\")\n",
    "\n",
    "# Compare with original best model\n",
    "original_auc = final_results_df.iloc[0]['Test_AUC']\n",
    "improvement = tuned_auc - original_auc\n",
    "print(f\"\\nImprovement from tuning: {improvement:.4f} ({improvement/original_auc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Your Turn to Practice!\n",
    "Now it's your turn to experiment with ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Create a Custom Ensemble\n",
    "Create a weighted voting ensemble where you assign different weights to different models based on their individual performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 1\n",
    "# Hint: You can manually combine predictions using weights based on individual model AUC scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Feature Importance Ensemble\n",
    "Compare feature importances across Random Forest, Gradient Boosting, and XGBoost. Which features are consistently important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 2\n",
    "# Hint: Extract feature_importances_ from each model and create a comparison DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Stacking Ensemble\n",
    "Create a simple stacking ensemble where you use the predictions of multiple models as features for a final meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 3\n",
    "# Hint: Use cross-validation to generate out-of-fold predictions, then train a meta-model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've mastered ensemble methods and advanced modeling techniques. Here's what you've learned:\n",
    "\n",
    "### ‚úÖ Key Skills Mastered:\n",
    "1. **Bagging Methods**: Random Forest and general bagging with different base estimators\n",
    "2. **Boosting Methods**: Gradient Boosting, XGBoost, and AdaBoost\n",
    "3. **Voting Ensembles**: Hard voting (majority) and soft voting (probability averaging)\n",
    "4. **Model Comparison**: Systematic evaluation of multiple ensemble approaches\n",
    "5. **Hyperparameter Tuning**: Grid search for optimal model parameters\n",
    "6. **Feature Importance**: Understanding which features drive ensemble predictions\n",
    "\n",
    "### üîç Key Concepts Learned:\n",
    "- **Bias-Variance Tradeoff**: Ensembles reduce variance (bagging) or bias (boosting)\n",
    "- **Model Diversity**: Different algorithms capture different patterns in data\n",
    "- **Overfitting Reduction**: Averaging multiple models reduces overfitting\n",
    "- **Computational Cost**: Ensembles trade computational resources for better performance\n",
    "- **Interpretability**: Ensemble models are less interpretable than individual models\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "In the next lab, we'll explore unsupervised learning techniques:\n",
    "- K-Means and hierarchical clustering\n",
    "- Principal Component Analysis (PCA)\n",
    "- Customer segmentation applications\n",
    "- Dimensionality reduction techniques\n",
    "\n",
    "### üìä Performance Insights:\n",
    "- Ensemble methods typically provide 2-5% improvement over single models\n",
    "- XGBoost and Random Forest are often top performers on tabular data\n",
    "- Soft voting usually outperforms hard voting\n",
    "- Hyperparameter tuning can provide additional 1-3% improvement\n",
    "- Model diversity is key to ensemble success\n",
    "\n",
    "### üìö Additional Resources:\n",
    "- [Ensemble Methods in Scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- [XGBoost Documentation](https://xgboost.readthedocs.io/)\n",
    "- [Random Forest Explained](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)\n",
    "- [Gradient Boosting Guide](https://towardsdatascience.com/gradient-boosting-classification-explained-through-python-60cc980eeb3d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

