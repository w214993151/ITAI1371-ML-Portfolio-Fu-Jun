{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: Clustering and Dimensionality Reduction\n",
    "## Module 9, Lab 6: Discovering Hidden Patterns in Data\n",
    "\n",
    "Unlike supervised learning where we have target labels, unsupervised learning finds hidden patterns in data without knowing the \"correct\" answers. This lab explores clustering techniques to group similar data points and dimensionality reduction to visualize complex data.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand the principles of unsupervised learning\n",
    "- Apply K-Means clustering to segment customers\n",
    "- Use hierarchical clustering for different grouping strategies\n",
    "- Evaluate clustering quality using various metrics\n",
    "- Apply Principal Component Analysis (PCA) for dimensionality reduction\n",
    "- Visualize high-dimensional data in 2D/3D space\n",
    "- Interpret business insights from clustering results\n",
    "\n",
    "### Business Applications\n",
    "Unsupervised learning is crucial for:\n",
    "- Customer segmentation for targeted marketing\n",
    "- Market research and product positioning\n",
    "- Anomaly detection in fraud prevention\n",
    "- Data exploration and pattern discovery\n",
    "- Dimensionality reduction for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Customer Dataset for Segmentation\n",
    "We'll create a realistic customer dataset with multiple dimensions for clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive customer dataset\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "# Create different customer segments with distinct characteristics\n",
    "# Segment 1: Young Professionals (25% of customers)\n",
    "n_young_prof = int(0.25 * n_customers)\n",
    "young_prof = {\n",
    "    'age': np.random.normal(28, 4, n_young_prof),\n",
    "    'income': np.random.normal(55000, 15000, n_young_prof),\n",
    "    'spending_score': np.random.normal(70, 15, n_young_prof),\n",
    "    'frequency_visits': np.random.normal(8, 2, n_young_prof),\n",
    "    'avg_transaction': np.random.normal(85, 20, n_young_prof),\n",
    "    'online_purchases': np.random.normal(12, 3, n_young_prof),\n",
    "    'loyalty_years': np.random.normal(2, 1, n_young_prof)\n",
    "}\n",
    "\n",
    "# Segment 2: Affluent Families (30% of customers)\n",
    "n_affluent = int(0.30 * n_customers)\n",
    "affluent = {\n",
    "    'age': np.random.normal(42, 8, n_affluent),\n",
    "    'income': np.random.normal(95000, 25000, n_affluent),\n",
    "    'spending_score': np.random.normal(85, 10, n_affluent),\n",
    "    'frequency_visits': np.random.normal(6, 2, n_affluent),\n",
    "    'avg_transaction': np.random.normal(150, 30, n_affluent),\n",
    "    'online_purchases': np.random.normal(8, 2, n_affluent),\n",
    "    'loyalty_years': np.random.normal(5, 2, n_affluent)\n",
    "}\n",
    "\n",
    "# Segment 3: Budget Conscious (25% of customers)\n",
    "n_budget = int(0.25 * n_customers)\n",
    "budget = {\n",
    "    'age': np.random.normal(35, 12, n_budget),\n",
    "    'income': np.random.normal(40000, 12000, n_budget),\n",
    "    'spending_score': np.random.normal(35, 10, n_budget),\n",
    "    'frequency_visits': np.random.normal(4, 1, n_budget),\n",
    "    'avg_transaction': np.random.normal(45, 15, n_budget),\n",
    "    'online_purchases': np.random.normal(3, 1, n_budget),\n",
    "    'loyalty_years': np.random.normal(3, 2, n_budget)\n",
    "}\n",
    "\n",
    "# Segment 4: Senior Customers (20% of customers)\n",
    "n_senior = n_customers - n_young_prof - n_affluent - n_budget\n",
    "senior = {\n",
    "    'age': np.random.normal(65, 8, n_senior),\n",
    "    'income': np.random.normal(70000, 20000, n_senior),\n",
    "    'spending_score': np.random.normal(50, 15, n_senior),\n",
    "    'frequency_visits': np.random.normal(3, 1, n_senior),\n",
    "    'avg_transaction': np.random.normal(75, 25, n_senior),\n",
    "    'online_purchases': np.random.normal(2, 1, n_senior),\n",
    "    'loyalty_years': np.random.normal(8, 3, n_senior)\n",
    "}\n",
    "\n",
    "# Combine all segments\n",
    "customer_data = {}\n",
    "for key in young_prof.keys():\n",
    "    customer_data[key] = np.concatenate([\n",
    "        young_prof[key], affluent[key], budget[key], senior[key]\n",
    "    ])\n",
    "\n",
    "# Add true segment labels (for evaluation purposes only)\n",
    "true_segments = (['Young Professional'] * n_young_prof + \n",
    "                ['Affluent Family'] * n_affluent + \n",
    "                ['Budget Conscious'] * n_budget + \n",
    "                ['Senior Customer'] * n_senior)\n",
    "\n",
    "customer_data['true_segment'] = true_segments\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(customer_data)\n",
    "\n",
    "# Apply realistic constraints\n",
    "df['age'] = np.clip(df['age'], 18, 80)\n",
    "df['income'] = np.clip(df['income'], 20000, 200000)\n",
    "df['spending_score'] = np.clip(df['spending_score'], 1, 100)\n",
    "df['frequency_visits'] = np.clip(df['frequency_visits'], 1, 15)\n",
    "df['avg_transaction'] = np.clip(df['avg_transaction'], 10, 300)\n",
    "df['online_purchases'] = np.clip(df['online_purchases'], 0, 20)\n",
    "df['loyalty_years'] = np.clip(df['loyalty_years'], 0, 15)\n",
    "\n",
    "# Add some derived features\n",
    "df['annual_spending'] = df['frequency_visits'] * df['avg_transaction'] * 12\n",
    "df['online_ratio'] = df['online_purchases'] / (df['frequency_visits'] + 1)\n",
    "df['spending_to_income'] = df['annual_spending'] / df['income']\n",
    "\n",
    "print(f\"Dataset created with {len(df)} customers\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nTrue segment distribution:\")\n",
    "print(df['true_segment'].value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Exploratory Data Analysis\n",
    "Let's explore our data before applying clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "numerical_cols = ['age', 'income', 'spending_score', 'frequency_visits', \n",
    "                 'avg_transaction', 'online_purchases', 'loyalty_years',\n",
    "                 'annual_spending', 'online_ratio', 'spending_to_income']\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(df[numerical_cols].describe().round(2))\n",
    "\n",
    "# Check for correlations\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions and relationships\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "key_features = ['age', 'income', 'spending_score', 'frequency_visits', 'avg_transaction', 'annual_spending']\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    axes[i].hist(df[feature], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'Distribution of {feature.replace(\"_\", \" \").title()}')\n",
    "    axes[i].set_xlabel(feature.replace(\"_\", \" \").title())\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots to visualize potential clusters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Age vs Income\n",
    "axes[0, 0].scatter(df['age'], df['income'], alpha=0.6, c=df['spending_score'], cmap='viridis')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Income')\n",
    "axes[0, 0].set_title('Age vs Income (colored by Spending Score)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Income vs Annual Spending\n",
    "axes[0, 1].scatter(df['income'], df['annual_spending'], alpha=0.6, c=df['age'], cmap='plasma')\n",
    "axes[0, 1].set_xlabel('Income')\n",
    "axes[0, 1].set_ylabel('Annual Spending')\n",
    "axes[0, 1].set_title('Income vs Annual Spending (colored by Age)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Spending Score vs Frequency\n",
    "axes[1, 0].scatter(df['spending_score'], df['frequency_visits'], alpha=0.6, c=df['loyalty_years'], cmap='coolwarm')\n",
    "axes[1, 0].set_xlabel('Spending Score')\n",
    "axes[1, 0].set_ylabel('Frequency of Visits')\n",
    "axes[1, 0].set_title('Spending Score vs Frequency (colored by Loyalty Years)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Online Purchases vs Age\n",
    "axes[1, 1].scatter(df['age'], df['online_purchases'], alpha=0.6, c=df['income'], cmap='viridis')\n",
    "axes[1, 1].set_xlabel('Age')\n",
    "axes[1, 1].set_ylabel('Online Purchases')\n",
    "axes[1, 1].set_title('Age vs Online Purchases (colored by Income)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing for Clustering\n",
    "Clustering algorithms are sensitive to feature scales, so we need to normalize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for clustering (exclude true_segment as it's our ground truth)\n",
    "clustering_features = ['age', 'income', 'spending_score', 'frequency_visits', \n",
    "                      'avg_transaction', 'online_purchases', 'loyalty_years']\n",
    "\n",
    "X = df[clustering_features].copy()\n",
    "\n",
    "print(f\"Features for clustering: {clustering_features}\")\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"\\nFeature ranges before scaling:\")\n",
    "for col in clustering_features:\n",
    "    print(f\"  {col}: {X[col].min():.2f} to {X[col].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply different scaling methods\n",
    "# StandardScaler (mean=0, std=1)\n",
    "scaler_standard = StandardScaler()\n",
    "X_standard = scaler_standard.fit_transform(X)\n",
    "X_standard_df = pd.DataFrame(X_standard, columns=clustering_features)\n",
    "\n",
    "# MinMaxScaler (range 0-1)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_minmax = scaler_minmax.fit_transform(X)\n",
    "X_minmax_df = pd.DataFrame(X_minmax, columns=clustering_features)\n",
    "\n",
    "print(\"Scaling applied successfully!\")\n",
    "print(f\"\\nStandard scaled data statistics:\")\n",
    "print(X_standard_df.describe().round(3))\n",
    "\n",
    "print(f\"\\nMin-Max scaled data statistics:\")\n",
    "print(X_minmax_df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of scaling\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original data\n",
    "X.boxplot(ax=axes[0])\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_ylabel('Values')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Standard scaled\n",
    "X_standard_df.boxplot(ax=axes[1])\n",
    "axes[1].set_title('Standard Scaled Data')\n",
    "axes[1].set_ylabel('Values')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Min-max scaled\n",
    "X_minmax_df.boxplot(ax=axes[2])\n",
    "axes[2].set_title('Min-Max Scaled Data')\n",
    "axes[2].set_ylabel('Values')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# We'll use standard scaled data for clustering\n",
    "X_scaled = X_standard\n",
    "print(\"\\nUsing standard scaled data for clustering analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: K-Means Clustering\n",
    "Let's start with K-Means, one of the most popular clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Finding the Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method - find optimal k\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "calinski_scores = []\n",
    "davies_bouldin_scores = []\n",
    "\n",
    "print(\"Finding optimal number of clusters...\")\n",
    "for k in k_range:\n",
    "    # Fit K-Means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, cluster_labels))\n",
    "    calinski_scores.append(calinski_harabasz_score(X_scaled, cluster_labels))\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(X_scaled, cluster_labels))\n",
    "    \n",
    "    print(f\"k={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_scores[-1]:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Elbow curve\n",
    "axes[0, 0].plot(k_range, inertias, 'bo-')\n",
    "axes[0, 0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0, 0].set_ylabel('Inertia (Within-cluster sum of squares)')\n",
    "axes[0, 0].set_title('Elbow Method')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette score\n",
    "axes[0, 1].plot(k_range, silhouette_scores, 'ro-')\n",
    "axes[0, 1].set_xlabel('Number of Clusters (k)')\n",
    "axes[0, 1].set_ylabel('Silhouette Score')\n",
    "axes[0, 1].set_title('Silhouette Analysis')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Calinski-Harabasz score\n",
    "axes[1, 0].plot(k_range, calinski_scores, 'go-')\n",
    "axes[1, 0].set_xlabel('Number of Clusters (k)')\n",
    "axes[1, 0].set_ylabel('Calinski-Harabasz Score')\n",
    "axes[1, 0].set_title('Calinski-Harabasz Index')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Davies-Bouldin score (lower is better)\n",
    "axes[1, 1].plot(k_range, davies_bouldin_scores, 'mo-')\n",
    "axes[1, 1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1, 1].set_ylabel('Davies-Bouldin Score')\n",
    "axes[1, 1].set_title('Davies-Bouldin Index (lower is better)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal k based on silhouette score\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nOptimal number of clusters based on Silhouette Score: {optimal_k}\")\n",
    "print(f\"Best Silhouette Score: {max(silhouette_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Apply K-Means with Optimal K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with optimal k\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans_optimal.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['kmeans_cluster'] = kmeans_labels\n",
    "\n",
    "print(f\"K-Means clustering completed with {optimal_k} clusters\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "cluster_counts = pd.Series(kmeans_labels).value_counts().sort_index()\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"  Cluster {cluster}: {count} customers ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Calculate final metrics\n",
    "final_silhouette = silhouette_score(X_scaled, kmeans_labels)\n",
    "final_calinski = calinski_harabasz_score(X_scaled, kmeans_labels)\n",
    "final_davies_bouldin = davies_bouldin_score(X_scaled, kmeans_labels)\n",
    "\n",
    "print(f\"\\nClustering Quality Metrics:\")\n",
    "print(f\"  Silhouette Score: {final_silhouette:.3f} (higher is better, range: -1 to 1)\")\n",
    "print(f\"  Calinski-Harabasz Score: {final_calinski:.2f} (higher is better)\")\n",
    "print(f\"  Davies-Bouldin Score: {final_davies_bouldin:.3f} (lower is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "cluster_analysis = df.groupby('kmeans_cluster')[clustering_features].mean().round(2)\n",
    "print(\"Cluster Characteristics (Mean Values):\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Visualize cluster characteristics\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(clustering_features):\n",
    "    cluster_means = cluster_analysis[feature]\n",
    "    axes[i].bar(range(len(cluster_means)), cluster_means.values, \n",
    "                color=plt.cm.Set3(np.linspace(0, 1, len(cluster_means))))\n",
    "    axes[i].set_title(f'{feature.replace(\"_\", \" \").title()} by Cluster')\n",
    "    axes[i].set_xlabel('Cluster')\n",
    "    axes[i].set_ylabel(feature.replace(\"_\", \" \").title())\n",
    "    axes[i].set_xticks(range(len(cluster_means)))\n",
    "    axes[i].set_xticklabels([f'C{i}' for i in range(len(cluster_means))])\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[7])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hierarchical Clustering\n",
    "Let's compare K-Means with hierarchical clustering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dendrogram to visualize hierarchical clustering\n",
    "# Use a subset of data for visualization (dendrograms can be messy with too many points)\n",
    "sample_size = 100\n",
    "sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
    "X_sample = X_scaled[sample_indices]\n",
    "\n",
    "# Calculate linkage matrix\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, method in enumerate(linkage_methods):\n",
    "    # Calculate linkage\n",
    "    Z = linkage(X_sample, method=method)\n",
    "    \n",
    "    # Create dendrogram\n",
    "    dendrogram(Z, ax=axes[i], truncate_mode='level', p=10)\n",
    "    axes[i].set_title(f'Dendrogram - {method.title()} Linkage')\n",
    "    axes[i].set_xlabel('Sample Index or (Cluster Size)')\n",
    "    axes[i].set_ylabel('Distance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
    "agg_labels = agg_clustering.fit_predict(X_scaled)\n",
    "\n",
    "# Add to dataframe\n",
    "df['hierarchical_cluster'] = agg_labels\n",
    "\n",
    "print(f\"Hierarchical clustering completed with {optimal_k} clusters\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "agg_cluster_counts = pd.Series(agg_labels).value_counts().sort_index()\n",
    "for cluster, count in agg_cluster_counts.items():\n",
    "    print(f\"  Cluster {cluster}: {count} customers ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Calculate metrics for hierarchical clustering\n",
    "agg_silhouette = silhouette_score(X_scaled, agg_labels)\n",
    "agg_calinski = calinski_harabasz_score(X_scaled, agg_labels)\n",
    "agg_davies_bouldin = davies_bouldin_score(X_scaled, agg_labels)\n",
    "\n",
    "print(f\"\\nHierarchical Clustering Quality Metrics:\")\n",
    "print(f\"  Silhouette Score: {agg_silhouette:.3f}\")\n",
    "print(f\"  Calinski-Harabasz Score: {agg_calinski:.2f}\")\n",
    "print(f\"  Davies-Bouldin Score: {agg_davies_bouldin:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Dimensionality Reduction and Visualization\n",
    "Let's use PCA and t-SNE to visualize our high-dimensional clusters in 2D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Analyze explained variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "print(\"PCA Analysis:\")\n",
    "print(f\"Number of original features: {X_scaled.shape[1]}\")\n",
    "for i, (var, cum_var) in enumerate(zip(explained_variance_ratio, cumulative_variance)):\n",
    "    print(f\"  PC{i+1}: {var:.3f} ({var*100:.1f}%) - Cumulative: {cum_var:.3f} ({cum_var*100:.1f}%)\")\n",
    "\n",
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Individual variance\n",
    "axes[0].bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Explained Variance by Principal Component')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n",
    "axes[1].axhline(y=0.8, color='r', linestyle='--', label='80% Variance')\n",
    "axes[1].axhline(y=0.95, color='g', linestyle='--', label='95% Variance')\n",
    "axes[1].set_xlabel('Number of Principal Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"\\nNumber of components needed for 95% variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze PCA components\n",
    "feature_names = clustering_features\n",
    "components_df = pd.DataFrame(\n",
    "    pca.components_[:3].T,  # First 3 components\n",
    "    columns=['PC1', 'PC2', 'PC3'],\n",
    "    index=feature_names\n",
    ")\n",
    "\n",
    "print(\"PCA Component Loadings (First 3 Components):\")\n",
    "print(components_df.round(3))\n",
    "\n",
    "# Visualize component loadings\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, pc in enumerate(['PC1', 'PC2', 'PC3']):\n",
    "    loadings = components_df[pc]\n",
    "    colors = ['red' if x < 0 else 'blue' for x in loadings]\n",
    "    axes[i].barh(range(len(loadings)), loadings, color=colors, alpha=0.7)\n",
    "    axes[i].set_yticks(range(len(loadings)))\n",
    "    axes[i].set_yticklabels(loadings.index)\n",
    "    axes[i].set_xlabel('Loading')\n",
    "    axes[i].set_title(f'{pc} Loadings\\n(Explained Variance: {explained_variance_ratio[i]:.1%})')\n",
    "    axes[i].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Cluster Visualization in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D visualizations of clusters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# K-Means clusters in PCA space\n",
    "scatter1 = axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=df['kmeans_cluster'], \n",
    "                             cmap='tab10', alpha=0.7, s=50)\n",
    "axes[0, 0].set_xlabel(f'PC1 ({explained_variance_ratio[0]:.1%} variance)')\n",
    "axes[0, 0].set_ylabel(f'PC2 ({explained_variance_ratio[1]:.1%} variance)')\n",
    "axes[0, 0].set_title('K-Means Clusters in PCA Space')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0, 0], label='K-Means Cluster')\n",
    "\n",
    "# Hierarchical clusters in PCA space\n",
    "scatter2 = axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=df['hierarchical_cluster'], \n",
    "                             cmap='tab10', alpha=0.7, s=50)\n",
    "axes[0, 1].set_xlabel(f'PC1 ({explained_variance_ratio[0]:.1%} variance)')\n",
    "axes[0, 1].set_ylabel(f'PC2 ({explained_variance_ratio[1]:.1%} variance)')\n",
    "axes[0, 1].set_title('Hierarchical Clusters in PCA Space')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[0, 1], label='Hierarchical Cluster')\n",
    "\n",
    "# True segments in PCA space (for comparison)\n",
    "true_segment_encoded = pd.Categorical(df['true_segment']).codes\n",
    "scatter3 = axes[1, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=true_segment_encoded, \n",
    "                             cmap='tab10', alpha=0.7, s=50)\n",
    "axes[1, 0].set_xlabel(f'PC1 ({explained_variance_ratio[0]:.1%} variance)')\n",
    "axes[1, 0].set_ylabel(f'PC2 ({explained_variance_ratio[1]:.1%} variance)')\n",
    "axes[1, 0].set_title('True Customer Segments in PCA Space')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter3, ax=axes[1, 0], label='True Segment')\n",
    "\n",
    "# 3D visualization using PC1, PC2, PC3\n",
    "ax_3d = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "scatter_3d = ax_3d.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], \n",
    "                          c=df['kmeans_cluster'], cmap='tab10', alpha=0.7, s=30)\n",
    "ax_3d.set_xlabel(f'PC1 ({explained_variance_ratio[0]:.1%})')\n",
    "ax_3d.set_ylabel(f'PC2 ({explained_variance_ratio[1]:.1%})')\n",
    "ax_3d.set_zlabel(f'PC3 ({explained_variance_ratio[2]:.1%})')\n",
    "ax_3d.set_title('K-Means Clusters in 3D PCA Space')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE for non-linear dimensionality reduction\n",
    "print(\"Applying t-SNE (this may take a moment...)\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Visualize clusters in t-SNE space\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# K-Means clusters\n",
    "scatter1 = axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=df['kmeans_cluster'], \n",
    "                          cmap='tab10', alpha=0.7, s=50)\n",
    "axes[0].set_xlabel('t-SNE 1')\n",
    "axes[0].set_ylabel('t-SNE 2')\n",
    "axes[0].set_title('K-Means Clusters in t-SNE Space')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='K-Means Cluster')\n",
    "\n",
    "# Hierarchical clusters\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=df['hierarchical_cluster'], \n",
    "                          cmap='tab10', alpha=0.7, s=50)\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "axes[1].set_title('Hierarchical Clusters in t-SNE Space')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Hierarchical Cluster')\n",
    "\n",
    "# True segments\n",
    "scatter3 = axes[2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=true_segment_encoded, \n",
    "                          cmap='tab10', alpha=0.7, s=50)\n",
    "axes[2].set_xlabel('t-SNE 1')\n",
    "axes[2].set_ylabel('t-SNE 2')\n",
    "axes[2].set_title('True Customer Segments in t-SNE Space')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter3, ax=axes[2], label='True Segment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"t-SNE visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Business Insights and Customer Segmentation Analysis\n",
    "Let's interpret our clustering results from a business perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed cluster analysis for business insights\n",
    "def analyze_clusters(df, cluster_column, cluster_name):\n",
    "    print(f\"\\n=== {cluster_name} ANALYSIS ===\")\n",
    "    \n",
    "    # Cluster sizes\n",
    "    cluster_sizes = df[cluster_column].value_counts().sort_index()\n",
    "    print(f\"\\nCluster Sizes:\")\n",
    "    for cluster, size in cluster_sizes.items():\n",
    "        print(f\"  Cluster {cluster}: {size} customers ({size/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Detailed statistics by cluster\n",
    "    cluster_stats = df.groupby(cluster_column)[clustering_features + ['annual_spending', 'spending_to_income']].agg({\n",
    "        'age': ['mean', 'std'],\n",
    "        'income': ['mean', 'std'],\n",
    "        'spending_score': ['mean', 'std'],\n",
    "        'frequency_visits': ['mean', 'std'],\n",
    "        'avg_transaction': ['mean', 'std'],\n",
    "        'online_purchases': ['mean', 'std'],\n",
    "        'loyalty_years': ['mean', 'std'],\n",
    "        'annual_spending': ['mean', 'std'],\n",
    "        'spending_to_income': ['mean', 'std']\n",
    "    }).round(2)\n",
    "    \n",
    "    print(f\"\\nDetailed Cluster Statistics:\")\n",
    "    print(cluster_stats)\n",
    "    \n",
    "    return cluster_stats\n",
    "\n",
    "# Analyze K-Means clusters\n",
    "kmeans_stats = analyze_clusters(df, 'kmeans_cluster', 'K-MEANS CLUSTERING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create business personas for each cluster\n",
    "def create_business_personas(df, cluster_column):\n",
    "    personas = {}\n",
    "    \n",
    "    for cluster in sorted(df[cluster_column].unique()):\n",
    "        cluster_data = df[df[cluster_column] == cluster]\n",
    "        \n",
    "        persona = {\n",
    "            'size': len(cluster_data),\n",
    "            'percentage': len(cluster_data) / len(df) * 100,\n",
    "            'avg_age': cluster_data['age'].mean(),\n",
    "            'avg_income': cluster_data['income'].mean(),\n",
    "            'avg_spending_score': cluster_data['spending_score'].mean(),\n",
    "            'avg_frequency': cluster_data['frequency_visits'].mean(),\n",
    "            'avg_transaction': cluster_data['avg_transaction'].mean(),\n",
    "            'avg_online': cluster_data['online_purchases'].mean(),\n",
    "            'avg_loyalty': cluster_data['loyalty_years'].mean(),\n",
    "            'total_annual_spending': cluster_data['annual_spending'].sum(),\n",
    "            'avg_annual_spending': cluster_data['annual_spending'].mean()\n",
    "        }\n",
    "        \n",
    "        personas[cluster] = persona\n",
    "    \n",
    "    return personas\n",
    "\n",
    "# Create personas\n",
    "kmeans_personas = create_business_personas(df, 'kmeans_cluster')\n",
    "\n",
    "print(\"\\n=== CUSTOMER PERSONAS (K-MEANS) ===\")\n",
    "for cluster, persona in kmeans_personas.items():\n",
    "    print(f\"\\nðŸŽ¯ CLUSTER {cluster} ({persona['percentage']:.1f}% of customers):\")\n",
    "    print(f\"   ðŸ‘¥ Size: {persona['size']} customers\")\n",
    "    print(f\"   ðŸ‘¤ Average Age: {persona['avg_age']:.1f} years\")\n",
    "    print(f\"   ðŸ’° Average Income: ${persona['avg_income']:,.0f}\")\n",
    "    print(f\"   ðŸ“Š Spending Score: {persona['avg_spending_score']:.1f}/100\")\n",
    "    print(f\"   ðŸ›ï¸ Visit Frequency: {persona['avg_frequency']:.1f} times/month\")\n",
    "    print(f\"   ðŸ’³ Average Transaction: ${persona['avg_transaction']:.0f}\")\n",
    "    print(f\"   ðŸŒ Online Purchases: {persona['avg_online']:.1f}/month\")\n",
    "    print(f\"   â­ Loyalty: {persona['avg_loyalty']:.1f} years\")\n",
    "    print(f\"   ðŸ’µ Annual Spending: ${persona['avg_annual_spending']:,.0f}\")\n",
    "    print(f\"   ðŸ“ˆ Total Segment Value: ${persona['total_annual_spending']:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business recommendations based on clustering\n",
    "def generate_business_recommendations(personas):\n",
    "    print(\"\\n=== BUSINESS RECOMMENDATIONS ===\")\n",
    "    \n",
    "    # Find highest value segments\n",
    "    segments_by_value = sorted(personas.items(), \n",
    "                              key=lambda x: x[1]['total_annual_spending'], \n",
    "                              reverse=True)\n",
    "    \n",
    "    print(f\"\\nðŸ’Ž SEGMENT PRIORITIZATION (by total value):\")\n",
    "    for i, (cluster, persona) in enumerate(segments_by_value, 1):\n",
    "        print(f\"   {i}. Cluster {cluster}: ${persona['total_annual_spending']:,.0f} total value\")\n",
    "    \n",
    "    # Specific recommendations for each cluster\n",
    "    print(f\"\\nðŸŽ¯ TARGETED STRATEGIES:\")\n",
    "    \n",
    "    for cluster, persona in personas.items():\n",
    "        print(f\"\\n   Cluster {cluster} Strategy:\")\n",
    "        \n",
    "        # High spenders\n",
    "        if persona['avg_annual_spending'] > 50000:\n",
    "            print(f\"     â€¢ VIP treatment and exclusive offers\")\n",
    "            print(f\"     â€¢ Premium product recommendations\")\n",
    "            print(f\"     â€¢ Personal shopping assistance\")\n",
    "        \n",
    "        # Frequent visitors\n",
    "        if persona['avg_frequency'] > 6:\n",
    "            print(f\"     â€¢ Loyalty rewards program\")\n",
    "            print(f\"     â€¢ Bulk purchase discounts\")\n",
    "        \n",
    "        # Online shoppers\n",
    "        if persona['avg_online'] > 5:\n",
    "            print(f\"     â€¢ Enhanced online experience\")\n",
    "            print(f\"     â€¢ Mobile app promotions\")\n",
    "            print(f\"     â€¢ Digital marketing campaigns\")\n",
    "        \n",
    "        # Young customers\n",
    "        if persona['avg_age'] < 35:\n",
    "            print(f\"     â€¢ Social media engagement\")\n",
    "            print(f\"     â€¢ Trendy product lines\")\n",
    "            print(f\"     â€¢ Referral programs\")\n",
    "        \n",
    "        # Older customers\n",
    "        if persona['avg_age'] > 55:\n",
    "            print(f\"     â€¢ Traditional marketing channels\")\n",
    "            print(f\"     â€¢ In-store assistance\")\n",
    "            print(f\"     â€¢ Quality and reliability focus\")\n",
    "        \n",
    "        # Low spenders\n",
    "        if persona['avg_annual_spending'] < 20000:\n",
    "            print(f\"     â€¢ Value-oriented promotions\")\n",
    "            print(f\"     â€¢ Budget-friendly product lines\")\n",
    "            print(f\"     â€¢ Gradual upselling strategies\")\n",
    "\n",
    "generate_business_recommendations(kmeans_personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clustering methods\n",
    "print(\"\\n=== CLUSTERING METHOD COMPARISON ===\")\n",
    "\n",
    "comparison_metrics = {\n",
    "    'K-Means': {\n",
    "        'Silhouette': final_silhouette,\n",
    "        'Calinski-Harabasz': final_calinski,\n",
    "        'Davies-Bouldin': final_davies_bouldin\n",
    "    },\n",
    "    'Hierarchical': {\n",
    "        'Silhouette': agg_silhouette,\n",
    "        'Calinski-Harabasz': agg_calinski,\n",
    "        'Davies-Bouldin': agg_davies_bouldin\n",
    "    }\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_metrics).T\n",
    "print(\"\\nClustering Quality Metrics Comparison:\")\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Determine best method\n",
    "best_silhouette = comparison_df['Silhouette'].idxmax()\n",
    "best_calinski = comparison_df['Calinski-Harabasz'].idxmax()\n",
    "best_davies = comparison_df['Davies-Bouldin'].idxmin()  # Lower is better\n",
    "\n",
    "print(f\"\\nðŸ† Best Performance:\")\n",
    "print(f\"   â€¢ Silhouette Score: {best_silhouette}\")\n",
    "print(f\"   â€¢ Calinski-Harabasz: {best_calinski}\")\n",
    "print(f\"   â€¢ Davies-Bouldin: {best_davies}\")\n",
    "\n",
    "# Overall recommendation\n",
    "if best_silhouette == best_calinski:\n",
    "    print(f\"\\nâœ… Recommended Method: {best_silhouette}\")\n",
    "    print(f\"   Consistently performs best across multiple metrics\")\n",
    "else:\n",
    "    print(f\"\\nâš–ï¸ Mixed Results: Consider business context and interpretability\")\n",
    "    print(f\"   K-Means: Generally faster and more interpretable\")\n",
    "    print(f\"   Hierarchical: Better for understanding cluster relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Your Turn to Practice!\n",
    "Now it's your turn to apply unsupervised learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: DBSCAN Clustering\n",
    "Apply DBSCAN clustering to the dataset and compare its results with K-Means. DBSCAN can find clusters of arbitrary shape and identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 1\n",
    "# Hint: Use DBSCAN from sklearn.cluster and experiment with eps and min_samples parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Feature Selection for Clustering\n",
    "Try clustering using only a subset of features (e.g., just age, income, and spending_score). How do the results change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 2\n",
    "# Hint: Select a subset of features and repeat the K-Means analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Cluster Validation\n",
    "Calculate the Adjusted Rand Index (ARI) to compare how well your clustering results match the true customer segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 3\n",
    "# Hint: Use adjusted_rand_score from sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've mastered unsupervised learning and clustering techniques. Here's what you've learned:\n",
    "\n",
    "### âœ… Key Skills Mastered:\n",
    "1. **K-Means Clustering**: Finding optimal clusters using elbow method and silhouette analysis\n",
    "2. **Hierarchical Clustering**: Understanding different linkage methods and dendrograms\n",
    "3. **Cluster Evaluation**: Using silhouette score, Calinski-Harabasz, and Davies-Bouldin metrics\n",
    "4. **Dimensionality Reduction**: PCA for linear and t-SNE for non-linear dimension reduction\n",
    "5. **Data Preprocessing**: Scaling features for clustering algorithms\n",
    "6. **Business Application**: Translating clusters into actionable customer segments\n",
    "7. **Visualization**: Creating meaningful 2D/3D representations of high-dimensional data\n",
    "\n",
    "### ðŸ” Key Concepts Learned:\n",
    "- **Unsupervised Learning**: Finding patterns without labeled data\n",
    "- **Cluster Quality**: Multiple metrics provide different perspectives on clustering success\n",
    "- **Feature Scaling**: Critical for distance-based algorithms like K-Means\n",
    "- **Dimensionality Curse**: High-dimensional data challenges and solutions\n",
    "- **Business Translation**: Converting statistical clusters into marketing personas\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "In the next lab, we'll explore fairness and bias in machine learning:\n",
    "- Understanding algorithmic bias\n",
    "- Measuring fairness in ML models\n",
    "- Bias mitigation techniques\n",
    "- Ethical considerations in AI\n",
    "\n",
    "### ðŸ’¼ Business Applications:\n",
    "- **Customer Segmentation**: Targeted marketing campaigns\n",
    "- **Market Research**: Understanding customer behavior patterns\n",
    "- **Product Development**: Identifying unmet customer needs\n",
    "- **Pricing Strategy**: Segment-based pricing models\n",
    "- **Resource Allocation**: Optimizing marketing spend across segments\n",
    "\n",
    "### ðŸ“š Additional Resources:\n",
    "- [Scikit-learn Clustering Guide](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "- [K-Means Clustering Explained](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)\n",
    "- [PCA Explained](https://towardsdatascience.com/a-step-by-step-explanation-of-principal-component-analysis-b836fb9c97e2)\n",
    "- [Customer Segmentation with Python](https://towardsdatascience.com/customer-segmentation-using-k-means-clustering-d33964f238c3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

